%!TEX root = main.tex

\section[Identical responses]{Inferring consequences when observations and consequences share identical responses}\label{sec:evaluating_decisions}

Recall to the example discussed in the introduction: an author wants to choose the genre of a book they will write. There, we proposed a causal model that predicted that the distribution of sales conditional on genre, the author's historical sales success and global trends sales does not change under intervention on genre.

We can postulate a stronger notion of invariance: not only does the distribution of sales conditional on genre and the covariates not change under intervention -- whatever that means, precisely -- but it doesn't change under \emph{any} of the author's available actions. In this case, the author can reason as follows: while they don't know exactly what consequences deciding to write a romance novel ($\alpha_{\mathrm{romance}}$) will have, they know that under this choice they are more likely to produce a romance novel vs deciding to write a science fiction novel ($\alpha_{\mathrm{sf}}$). They also know that their choice will not affect their history of book sales, nor recent global trends in sales. Thus, despite their uncertainty over the details of the consequences, choosing to write a romance novel will lead to more sales in expectation.

This assumption allows the author to identify ``choosing to write a romance novel'' with a causal intervention on genre. It cannot always hold -- the author could, after all, choose to write the worst romance novel that they can imagine, which should surely be expected to sell worse than a novel that they attempt to make good. We will discuss the possibility of unusual or pathological actions further in Section \ref{sec:precedent}. If unusual plans present an obstacle to inference, the author may decide not to consider them. Even if the author avoids considering unusual plans, it is still unclear that it is reasonable to \emph{assume} that conditionals cannot change in this manner. Nevertheless, it forms the basis of a simple and important inference rule. For example, in the interventionist framework, conditionals invariant to intervention play a role in every identification result \citep{richardson_nested_2017}. For this reason, we think it is still worth studying.

Here is a mathematical formulation of our assumption: the author has a decision model $(\prob{P}_{\cdot}, (C,\sigalg{C}), (\Omega,\sigalg{F}))$ together with a sequence of random variables $(\RV{S}_i, \RV{G}_i, \RV{V}_i)_{i\in [m]\cup \{q\}}$ where the indices $[m]$ refer to the books observed in the dataset so far, and the special index $q$ refers to the book the author is hoping to write. The assumption we are discussing is that there is some unknown stochastic function, which we will call $\RV{H}$ (we represent unknown quantities with random variables), such that for all $\alpha$, $i$, $j$
\begin{align}
    \prob{P}_\alpha^{\RV{S}_i|\RV{G}_i\RV{V}_j\RV{H}} &= \prob{P}_\alpha^{\RV{S_j}|\RV{G}_j\RV{V}_j\RV{H}} = \RV{H}
\end{align}
we call $\RV{H}$ a ``response function'' because it models the way $\RV{S}_i$ responds to $\RV{G}_i$ and $\RV{V}_i$. We also assume that once we know $\RV{H}$, we have nothing more to learn about this response from observing more $(\RV{S}_i, \RV{G}_i, \RV{V}_i)$ triples. That is, we assume $\RV{S}_i \CI (\RV{S}_{\{i\}^\complement}, \RV{V}_{\{i\}^\complement}, \RV{G}_{\{i\}^\complement}) | (\RV{G}_i, \RV{V}_i, \RV{H}, \RV{C})$.

Thus this assumption has two components: first, conditional on $\RV{H}$, the response of every $\RV{S}_i$ to $(\RV{G}_i, \RV{V}_i)$ is identical regardless of $i$ and secondly, conditional on $\RV{H}$, $\RV{S}_i$ is independent of other triples $(\RV{S}_i, \RV{G}_i, \RV{V}_i)$. It is the assumption of \emph{conditionally independent and identical responses} (CIIR for short).

Is it ever reasonable to assume CIIR? Sometimes it might be -- perhaps the system being modelled has been deliberately engineered for regularity. A switch reliably turns on a light if you flick it, and a function in a piece of code reliably returns the same result given the same input. Book sales or human health are not examples of systems like this, however. Alternatively, we might suppose that everything of interest follows classical physics, and so in principle if we knew the right details of the initial condition in each case, we would observe a regular relationship between initial conditions and final states. This does not tell us whether any particular collection of variables should be expected to demonstrate a fixed response relationship.

Instead of appealing to our prior knowledge of mechanisms as we do in the case of engineered systems, we could try to appeal to knowledge of symmetries of the problem. The inspiration for this approach comes from De Finetti's work on Bayesian probabilistic inference. De Finetti, observing that many statistical models assumed a sequence of independent and identically distributed (IID) random variables conditional on an unknown ``true parameter'', argued that this assumption usually did not make much sense: it is often hard to convincingly argue for the existence of a mechanism that behaves in this manner. Instead, he suggested, we could appeal to the symmetry of our knowledge about a problem. Suppose, as far as we are concerned, the problem is not changed in any important way by permuting the measurements we have taken. De Finetti showed that the class of probability models with this symmetry (called \emph{exchangeability}) is equivalent to the class of models of IID random variables conditional on an unknown parameter \citep{de_finetti_foresight_1992}. De Finetti's argument is controversial -- for example, \citet{walley_statistical_1991} has argued that it does not under imprecise probability models. Nevertheless, it gave us substantial insight into the assumptions underpinning IID statistical models, and a similar result may help us understand CIIR decision models.

That is the line of thinking we pursue in this section. In particular, we prove a similar result to De Finetti's -- that result is that the class of CIIR decision models is equivalent to decision models with a symmetry we call \emph{input-output contractibility} (or IO contractibility). However, we are unable to come up with any new arguments for assuming CIIR on the basis of this result. IO contractibility is a less intuitive property than exchangeability, so we have overlooked some possibilities. We are able to provide an additional argument \emph{against} assumiming CIIR in general. IO contractibility implies that, after having seen infinite data, input-output pairs can be exchanged. Thus, if the author assumes CIIR they must accept that these two problems are identical:
\begin{itemize}
    \item Observe an infinite dataset of book sales and predict the sales of one more book in the dataset after observing its genre and covariates
    \item Observe the same infinite dataset of book sales and predict the sales of their own book after observing its genre and covariates
\end{itemize}
However, we think that these problems will generally not be identical -- the second problem will generally be harder than the first. Alternatively, suppose that instead of choosing a genre for 1 book, the author chooses a genre for $1000$ books, which are all launched at the same moment (so sales history & genre trends are held constant). In this case, the author is perfectly happy to disregard the obsrvations of 999 of their own books in order to predict the sales of their 1000th. The CIIR assumption is very dogmatic -- it doesn't just treat responses as identical before any evidence to the contrary is obtained, it even continues to insist they are the same once such evidence begins to accumulate. This assumption is far too strong for the many cases where we do not have clear reasons to assume CIIR. We need weaker assumptions if we are to consider them plausible, and we will discuss one candidate in the next section.

\subsection[CIIR sequences]{Conditionally independent and identical responses}\label{sec:response_functions}

We now turn to the formal treatment of the CIIR assumption, and its equivalence to IO contractibility. First, we define sequential input-output models as a shorthand for a decision model along with a sequence of random variable pairs.

\begin{definition}[Sequential input-output model]\label{def:seq_io}
A decision model $(\prob{P}_{\cdot}, (C,\sigalg{C}), (\Omega,\sigalg{F}))$ and two sequences of variables $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ and $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ is a sequential input-output model, which we specify with the shorthand $(\prob{P}_\cdot,\RV{D},\RV{Y})$.
\end{definition}

Sequential input-output pairs $(\RV{D}_i,\RV{Y}_i)_{i\in \mathbb{N}}$ share conditionally independent and identical responses if there is an unknown stochastic function $\RV{H}$ taking values in $\Delta(Y)^D$ -- that is, in the set of maps from $D$ to probability distributions over $Y$ -- such that every output $\RV{Y}_i$ ``responds to'' $\RV{D}_i$ according to the same $\RV{H}$. Note that this assumption does not specify any relationship between options and the behaviour of inputs $\RV{D}_i$. The CIIR assumption is useful when the decision maker has some knowledge about how to control some inputs, but whether or not they have such knowledge is a separate question not relevant to this analysis.

\begin{definition}[Conditionally independent and identical responses]\label{def:cii_rf}
Given a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ along with some random variable $\RV{V}$, the $(\RV{D}_i,\RV{Y}_i)$ pairs are related by \emph{independent and identical responses conditional on} $\RV{H}$ if for all $i$, $\RV{Y}_i\CI (\RV{D}_{[1,i)},\RV{Y}_{[1,i)})|(\RV{D}_i,\RV{H},\RV{C})$ and $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{H}}\overset{C}{\cong}\prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j\RV{H}}$ for all $i,j$.
\end{definition}

% Definition \ref{def:cii_rf} asserts that there are versions of all the conditional distributions $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{V}}$ that are pairwise almost surely equal (recall that $\overset{C}{\cong}$ is short hand for ``almost surely equal for all $\alpha\in C$''). Theorem \ref{th:repr_cond} shows that this is sufficient for the existence of a single conditional distribution that is a version of $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{V}}$ for all $i$.

% % \begin{theorem}[Existence of representative conditional distribution]\label{th:repr_cond}
% % Given a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$, if the $(\RV{D}_i,\RV{Y}_i)$ pairs are related by independent and identical responses conditional on $\RV{V}$, define $\prob{P}_\alpha^{\RV{Y}_i|\RV{X}_i\RV{H}}(\cdot|h)\overset{C}{\cong} h^Y_X$ for all $i$.

% % We refer to the function $\RV{H}^Y_X:h\mapsto h^Y_X$ as a \emph{representative conditional distribution}.
% % \end{theorem}

% % \begin{proof}
% % Fix $v$, $\alpha$ and take $h^Y_{X,i}:=\prob{P}_\alpha^{\RV{Y}_i|\RV{X}_i\RV{H}}(\cdot|\cdot,h)$ to be an arbitrary version of the conditional distribution for all $i$.

% % For $i,j\in \mathbb{N}$, take $S_{ij} := \{x|h^Y_{x,i}\text{ is not a version of }\prob{P}_\alpha^{\RV{Y}_j|\RV{X}_j\RV{H}}(\cdot|\cdot,h)\}$. Note that $S_i:= \cup_{j\in \mathbb{N}} S_{ij}$ is a countable union of sets of $\prob{P}_\alpha^{\RV{X}_i|\RV{H}}(\cdot|h)$-measure 0, hence is also a set of $\prob{P}_\alpha^{\RV{X}_i|\RV{H}}(\cdot|h)$-measure 0.

% % Define
% % \begin{align}
% %     h^Y_X(A|x) := \sum_{i\in \mathh{N}} \mathds{1}_{S_i^\complement\setminus \cup_{j\in[i]}S_j^\complement}(x) h^Y_{X,i}(A|x)
% % \end{align}

% % By construction, $h^Y_X$ differs from each $h^Y_{X,i}$ by a measure 0 set with respect to $\prob{P}_\alpha^{\RV{X}_i|\RV{H}}(\cdot|h)$. Hence it is a version of $\prob{P}_\alpha^{\RV{Y}_i|\RV{X}_i\RV{H}}(\cdot|h)$ for every $i$.
% % \end{proof}

Note that in general, we only require the outputs $\RV{Y}_i$ be independent of \emph{previous} inputs and outputs conditional on $\RV{H}$ and $\RV{D}_i$. The reason for this is that, if we suppose that the variable indices match the time-ordering of variables, it's possible that $\RV{D}_i$ is chosen based on previous data (i.e. some author in the dataset might have chosen the genre of \emph{their} book based on their previous observations). This means that, in general, there may be relationships between $\RV{D}_j$ and $\RV{Y}_i$ for $j>i$ even after conditioning on $\RV{D}_i$ and $\RV{H}$. However, for our purposes we will use a stronger assumption that we call \emph{weak data-independence}, which means that conditional on $\RV{H}$ and past inputs $\RV{D}_{[1,i]}$, $\RV{Y}_i$ is also independent of all future inputs. Generalising our result to data-dependent inputs is an open question.

\begin{definition}[Weakly data-independent]\label{def:weak_di}
A sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with independent and identical responses conditional on $\RV{H}$ is weakly data-independent if $\RV{Y}_i\CI \RV{D}_{\{i\}^\complement}|(\RV{D}_{i},\RV{H},\RV{C})$.
\end{definition}

\subsection[Conditional probability symmetries]{Symmetries of sequential conditional probabilities}\label{sec:ccontracibility}

Given the previously mentioned sequences $\RV{D}$ and $\RV{Y}$, the decision maker has for each option $\alpha\in C$ a conditional probability $\prob{P}_\alpha^{\RV{Y}|\RV{D}}$ (note the absence of $\RV{H}$ in the conditioning arguments). In general, we don't have $\RV{Y}_i\CI (\RV{Y}_{\{i\}^\complement},\RV{D}_{\{i\}^\complement}) | \RV{D}_i$, because additional examples of the input-output relationship enable the decision maker to learn about the ``true form'' of the relationship in more detail. However, one way that they might consider one pair $(\RV{D}_i,\RV{Y}_i)$ to be ``essentially the same'' as $(\RV{D}_j, \RV{Y}_j)$ is if there is no effective difference made by swapping the pairs, so $\prob{P}_\alpha^{\RV{Y}_i\RV{Y}_j|\RV{D}_i\RV{D}_j}$ is essentially the same as $\prob{P}_\alpha^{\RV{Y}_j\RV{Y}_i|\RV{D}_j\RV{D}_i}$. Or, more generally, given any permutation $\rho:\mathbb{N}\to \mathbb{N}$, define $\RV{Y}_\rho:=(\RV{Y}_{\rho(i)})_{i\in\mathbb{N}}$ and $\RV{D}_{\rho}$ similarly. Then we could propose a symmetry such that for all $\alpha$, $\rho$

\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{D}} &= \prob{P}_\alpha^{\RV{Y}_\rho|\RV{D}_\rho}
\end{align}

This symmetry is reminiscent of exchangeability, and in Theorem \ref{lem:exch_prod_ciid} we show that it implies that the $(\RV{D}_i,\RV{Y}_i)$ share conditionally independent and identical responses. However, the converse is not true. The reason for this is that we assume the responses are identical, but we don't require that the inputs are identical. Thus it may be the case that we learn more from $\RV{D}_j$ than we do from $\RV{D}_i$. Example \ref{ex:no_swapping} shows this in more detail.

\begin{example}\label{ex:no_swapping}
Suppose there is a machine with two arms $D=\{0,1\}$, one of which always pays out \$100 and the other that pays out nothing. A decision maker (DM) doesn't know which is which, but DM watches two people operate the machine. The first person in the sequence knows exactly which arm is good, and the second one has no idea. The first person will always pull the good arm, while the second person will pull the good arm $50\%$ of the time. The response $\RV{H}$ takes values that can be summarised as ``0 is good'' and ``1 is good'' (which we'll just refer to as $\{0,1\}$), and the DM assigns 50\% probability to each initially. Then for any $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2\RV{D}_1}(100|1, 0) &= \prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,0)\prob{P}_\alpha^{\RV{H}|\RV{D}_2\RV{D}_1}(0|1,0) + \prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,1)\prob{P}_\alpha^{\RV{H}|\RV{D}_2\RV{D}_1}(1|1, 0)\\
    &= 0\cdot 1 + 1\cdot 0\\
    &= 0
\end{align}
while
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1\RV{D}_2}(100|1,0) &= \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1\RV{H}}(100|1,0)\prob{P}_\alpha^{\RV{H}|\RV{D}_1\RV{D}_2}(0|1) + \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1\RV{H}}(100|1,1)\prob{P}_\alpha^{\RV{H}|\RV{D}_1\RV{D}_2}(1|1,0)\\
    &= 0\cdot0 + 1\cdot 1\\
    &= 1\\
    &\neq \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{D}_1}(100|1,0)
\end{align}
\end{example}

Example \ref{ex:no_swapping} motivates the weaker symmetry we call \emph{exchange commutativity}. The key difference is that exchange commutativity allows for the permutation of pairs after conditioning on some variable $\RV{W}$. That is, a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is exchange commutative if there is some variable $\RV{W}$ such that the conditional $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is symmetric to paired swaps of $\RV{Y}$ and $\RV{D}$.

\begin{definition}[Exchange commutativity]\label{def:caus_exch}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$, we say $(\prob{P}_C,\RV{D},\RV{Y})$ \emph{commutes with exchange} over $\RV{W}$ if for all finite permutations $\rho:\mathbb{N}\to\mathbb{N}$ and all $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &=  \prob{P}_\alpha^{\RV{Y}_\rho|\RV{WD}_\rho}
\end{align}
We say $(\prob{P}_C,\RV{D},\RV{Y})$ commutes with exchange if there is some $\RV{W}$ such that $(\prob{P}_C,\RV{D},\RV{Y})$ commutes with exchange over $\RV{W}$.   
\end{definition}

A second regularity condition we will consider can be roughly understood as the idea that $\RV{Y}_i$ doesn't ``depend on'' $\RV{D}_j$ for $j\neq i$. As Example \ref{ex:no_swapping} suggests, this cannot be an assumption that $\RV{Y}_i$ doesn't depend on $\RV{D}_j$ unconditionally; $\RV{D}_j$ could, after all, offer some evidence about the state of the unknown response function $\RV{H}$. Instead, we assume that $\RV{Y}_i$ doesn't depend on non-corresponding $\RV{X}_j$ after conditioning on some auxiliary $\RV{W}$.

\begin{definition}[Locality]\label{def:caus_cont}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$, the model is \emph{local} over $\RV{W}$ if for all $\alpha\in C$, $n\in \mathbb{N}$, $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{X}_{\{i,\infty)}|(\RV{W},\RV{X}_i,\text{id}_C)$. If there is some $\RV{W}$ such that $(\prob{P}_C,\RV{D},\RV{Y})$ is local over $\RV{W}$ then we say $(\prob{P}_C,\RV{D},\RV{Y})$ is local.
\end{definition}

If an input-output model is both exchange commutative and local, then we say it is \emph{input-output contractible}. This term is chosen because such a model is unchanged by contractions of the input and output indices - see Theorem \ref{th:equal_of_condits}.

\begin{definition}[Input-output contractibility]\label{def:ccontract}
A sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$ is \emph{input-output contractible} (IO contractible) over $\RV{W}$ if it is local and commutes with exchange over $\RV{W}$.
\end{definition}

\begin{theorem}[Equality of equally sized conditionals]\label{th:equal_of_condits}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ and some $\RV{W}$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$ if and only if for all subsequences $A,B\subset \mathbb{N}$ with $|A|=|B|$ and for every $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_{A,\mathbb{N}\setminus A}} &= \prob{P}_\alpha^{\RV{Y}_B|\RV{WD}_{B,\mathbb{N}\setminus B}}\\
    &= \prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_A}\otimes \text{del}_{D^{|\mathbb{N}\setminus A|}}
\end{align}
\end{theorem}

\begin{proof}
Appendix \ref{sec:equal_condits}
\end{proof}

Appendix \ref{app:examples_symmetries} explores out two additional properties of these two symmetries. Example \ref{ex:no_implication} shows that neither locality nor exchange commutativity is implied by the other. Example \ref{ex:interference_w_locality} shows that locality by itself does not rule out everything that we might intuitively describe as ``interference'' between pairs.

We might wonder if both locality and exchange commutativity are needed, seeing as exchange commutativity itself appears to be a generalisation of exchangeability - and in fact, if we take the inputs to be trivial $*$ then it coincides precisely with exchangeability. However, for nontrivial inputs, we can construct exchance commutative models where the response function depends on a symmetric function of the full set of inputs $\RV{D}_i$. An example of this possibility is a crude model of inflation: if you give any one person \$100, they'll be \$100 richer in real terms, but if you give everyone \$100 you cause a lot of inflation so the benefits are reduced and may even be negative to sufficiently wealthy people. That is, the impact on someone's wealth $\RV{Y}_i$ doesn't just depend on $\RV{D}_i$, but on the entire sequence $\RV{D} = (\RV{D}_i)_{i\in [n]}$. In this model it doesn't make much sense to give half of the people \$100 then measure the effects, then estimate the impact on the remaining half, because the second half of the inputs change the final outcomes for the first half of the distribution.

\subsection[Representation]{Representation of IO contractible models}\label{sec:rep_theorem}

In this section, we state Theorem \ref{th:ciid_rep_kernel}: a sequential input output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ features pairs $(\RV{D}_i,\RV{Y}_i)$ related by conditionally independent and identical responses if and only if it is IO contractible over some variable $\RV{W}$.

The proof of the theorem is involved, and can be found in its entirety Appendix \ref{app:representation_proof}. Note that we employ a string diagram notation in some steps of the proof, explained in Appendix \ref{ssec:mken_diagrams}. In the main paper, we just introduce enough to explain the key terms in the theorem statement. 

\subsection{Preliminaries}\label{sec:rep_theorem_background}

\begin{definition}[Input count variable]\label{def:count_of_inputs}
Given a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ with countable $D$, $\#_{j}^k$ is the variable
\begin{align}
    \#_{\RV{D}_{\cdot}=j}^k := \sum_{i=1}^{k-1} \llbracket \RV{D}_i = j \rrbracket
\end{align}
That is, $\#_{\RV{D}_{\cdot}=j}^k$ is equal to the number of times $\RV{D}_i=j$ over all $i<k$.
\end{definition}

If we have an infinite sequence of pairs $(\RV{D}_i,\RV{Y}_i)$, we can wrap the sequence $\RV{Y}$ into a table $\RV{Y}^D$ such that $\RV{Y}^D_{11}$ is equal to the value of the first $\RV{Y}_i$ such that $\RV{D}_i=1$, $\RV{Y}^D_{21}$ is equal to the value of the second such $\RV{Y}_i$ and so forth. We call it a ``tabulated conditional'' because, under the assumption of CIIRs, we can evaluate a conditional $\prob{P}_\alpha^{\RV{Y}|\RV{D}}(\cdot|d_1,d_2,...)$ by ``looking up'' the marginal distribution $\prob{P}_\alpha^{\RV{Y}^D_{1 d_1}\RV{Y}^D_{2 d_2}...}$ over the appropriate elements of $\RV{Y}^D$.

\begin{definition}[Tabulated conditional distribution]\label{def:tab_cd}
Given a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ on $(\Omega,\sigalg{F})$, define the \emph{tabulated conditional distribution} $\RV{Y}^D:\Omega\to Y^{\mathbb{N}\times D}$ by
\begin{align}
    \RV{Y}^D_{ij} = \sum_{k=1}^{\infty} \llbracket \#_{\RV{D}_{\cdot}=j}^k = i\rrbracket \llbracket \RV{D}_k = j \rrbracket \RV{Y}_k
\end{align}
That is, the $(i,j)$-th coordinate of $\RV{Y}^D$ is equal to the value of $\RV{Y}_k$ for which the corresponding $\RV{D}_k$ is the $i$th instance of the value $j$ in the sequence $(\RV{D}_1,\RV{D}_2,...)$, or 0 if there are fewer than $i$ instances of $j$ in this sequence.
\end{definition}

The \emph{directing random measure} of a sequence of exchangeable variables is defined as the map from the set of events of each variable in the sequence the limit of normalised partial sums of indicator functions over that set \citep{kallenberg_basic_2005}. The directing random measure is a probability measure. For completeness, we also define a directing random measure in the case that the relevant limit does not exist, although we are only practically interested in using the definition where the limit does exist.

\begin{definition}[Directing random measure]\label{def:dir_rand_meas}
Given a decision model $(\prob{P}_\cdot,\Omega,\sigalg{F})$ and a sequence $\RV{X}:=(\RV{X}_i)_{i\in\mathbb{N}}$, the directing random measure of $\RV{X}$ written $\RV{H}:\Omega\to \Delta(X)$ is the function
\begin{align}
    \RV{H} := A \mapsto \begin{cases}
    \lim_{n\to \infty}\frac{1}{n} \sum_{i=1}^{\infty} \mathds{1}_{A}(\RV{X}_{i}) & \text{this limit exists for all }\alpha\in C\\
    \llbracket A = X \rrbracket &\text{otherwise}
    \end{cases} 
\end{align}
\end{definition}

Given input and output sequences $\RV{D}$ and $\RV{Y}$ we define the \emph{directing random conditional} as the directing random measure of the tabulated conditional $\RV{Y}^D$ interpreted as a sequence of column vectors $((\RV{Y}^D_{1j})_{j\in D},(\RV{Y}^D_{2j})_{j\in D},...)$.

\begin{definition}[Directing random conditional]\label{def:dir_rand_cond}
Given a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$, we will say the directing random conditional $\RV{H}:\Omega\to \Delta(Y^D)$ is the function
\begin{align}
    \RV{H} := \bigtimes_{j\in D} A_j \mapsto \begin{cases}
    \lim_{n\to \infty}\frac{1}{n} \sum_{i=1}^{\infty} \prod_{j\in D} \mathds{1}_{A_j}(\RV{Y}^D_{ij}) & \text{this limit exists}\\
    \llbracket \bigtimes_{j\in D} A_j = Y^D \rrbracket &\text{otherwise}
    \end{cases} 
\end{align}
\end{definition}

A finite permutation of rows is a function that independently permutes a finite number of elements in each row of a table. A special case of such a function is one that swaps entire columns (that is, a permutation of rows that applies the same permutation to each row).

\begin{definition}[Permutation of rows]
Given a sequence of indices $(i,j)_{i\in \mathbb{N},j\in D}$ a finite permutation of rows is a function $\eta:\mathbb{N}\times D\to \mathbb{N}\times D$ such that for each $j\in D$, $\eta_j:=\eta(\cdot,j)$ is a finite permutation $\mathbb{N}\to \mathbb{N}$ and $\eta(i,j)=(\eta_j(i),j)$.
\end{definition}

Lemma \ref{th:table_rep_kernel} shows that an IO contractible conditional distribution can be represented as the product of a column exchangeable probability distribution and a ``lookup function'' or ``switch''. This lookup function is also used in the representation of potential outcomes models (see, for example, \citet{rubin_causal_2005}), but we do not assume that the tabulated conditional $\RV{Y}^D$ is interpretable as potential outcomes. By representing a conditional probability as an exchangeable regular probability distribution, we can apply De Finetti's theorem, which is a key step in proving the main result of Theorem \ref{th:ciid_rep_kernel}.

To prove Lemma \ref{th:table_rep_kernel}, we assume that the set of input sequences in which each value appears infinitely often has measure 1 for every option in $C$. Without this assumption, we would have to accept positive probability that we run out of $\RV{D}_i$s taking some value $j\in D$ preventing us from filling out the ``tabulated conditional'' $\RV{Y}^D$ correctly. We call this side condition \emph{infinite support}.

\begin{definition}[Almost surely infinite]\label{def:infinite_support}
Given a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ with $D$ countable if, letting $E\subset D^{\mathbb{N}}$ be the set of all sequences such that for all $j\in D$
\begin{align}
    x\in E \implies \sum_{i=0}\llbracket x_i = j\rrbracket = \infty
\end{align}
we have $\prob{P}_\alpha^{\RV{D}|\RV{W}}(E|w)=1$ for all $\alpha,w$, then we say $\RV{D}$ is \emph{almost surely infinite } over $\RV{W}$.
\end{definition}

The key property of the tabulated conditional is that we can evaluate the regular conditional $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ by ``looking up'' the appropriate marginal of $\prob{P}_\alpha^{\RV{Y}^D}$.

\begin{lemma}\label{th:table_rep_kernel}
Suppose a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ is given with $D$ countable and $\RV{D}$ infinitely supported over $\RV{W}$. Then for some $\RV{W}$, $\alpha$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible if and only if
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(\bigtimes_{i\in \mathbb{N}}A_i|w,(d_i)_{i\in \mathbb{N}}) &= \prob{P}_\alpha^{(\RV{Y}^D_{i d_i})_{i\in\mathbb{N}}|\RV{W}}(\bigtimes_{i\in \mathbb{N}}A_i|w)&\forall A_i\in \sigalg{Y}^{D}, w\in W, d_i\in D
\end{align}
and for any finite permutation of rows $\eta:\mathbb{N}\times D\to \mathbb{N}\times D$
\begin{align}
    \prob{P}_\alpha^{(\RV{Y}^D_{ij})_{\mathbb{N}\times D}|\RV{W}}&= \prob{P}_\alpha^{(\RV{Y}^D_{\eta(i,j)})_{\mathbb{N}\times D}|\RV{W}}\label{eq:col_exch}
\end{align}
\end{lemma}

\begin{proof}
Only if: We define a random invertible function $\RV{R}:\Omega\times \mathbb{N}\to \mathbb{N}\times {D}$ that reorders the indicies so that, for $i\in \mathbb{N},j\in D$, $\RV{D}_{\RV{R}^{-1}(i,j)}=j$ almost surely. We then use IO contractibility to show that $\prob{P}_\alpha^{\RV{Y}|\RV{D}}(\cdot|d)$ is equal to the distribution of the elements of $\RV{Y}^D$ selected according to $d\in D^{\mathbb{N}}$.

If: We construct a conditional probability according to Definition \ref{def:tab_cd} and verify that it satisfies IO contractibility.

The full proof can be found in Appendix \ref{app:representation_proof}. Note that the proof uses string diagram notation explained in Appendix \ref{ssec:mken_diagrams}.
\end{proof}

Because the distribution $\prob{P}_\alpha^{\RV{Y}^D|\RV{W}}$ from Lemma \ref{th:table_rep_kernel} is row-exchangeable, the limit in the definition of the directing random conditional $\RV{H}$ exists almost surely (see Lemma \ref{lem:ciid_yd}).  In fact, we do not need the full sequence of pairs $(\RV{D},\RV{Y})$ to calculate $\RV{H}$; any subsequence $A\subset\mathbb{N}$ that satisfies the condition that $\RV{D}_A$ is infinitely supported over $\RV{W}$ is sufficient.

\begin{theorem}\label{th:any_infinite_sequence}
Suppose a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ is given with $D$ countable,  $\RV{D}$ infinitely supported over $\RV{W}$ and for some $\RV{W}$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible for all $\alpha$. Consider an infinite set $A\subset \mathbb{N}$, and let $\RV{D}_A:=(\RV{D}_i)_{i\in A}$ and $\RV{Y}_A:=(\RV{Y}_i)_{i\in A}$ such that $\RV{D}_A$ is also infinitely supported over $\RV{W}$. Then $\RV{H}_A$, the directing random conditional of $(\prob{P}_\cdot,\RV{D}_A,\RV{Y}_A)$ is almost surely equal to $\RV{H}$, the directing random conditional of $(\prob{P}_\cdot,\RV{D},\RV{Y})$.
\end{theorem}

\begin{proof}
The strategy we pursue is to show that an arbitrary subsequence of $(\RV{D}_i,\RV{Y}_i)$ pairs induces a random contraction of the rows of $\RV{Y}^D$. Then we show that the contracted version of $\RV{Y}^D$ has the same distribution as the original, and consequently the normalised partial sums converge to the same limit.

The proof is in Appendix \ref{app:representation_proof}.
\end{proof}

We are now ready to state the main result, Theorem \ref{th:ciid_rep_kernel}. Assuming a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ (Definition \ref{def:seq_io}) with inputs $\RV{D}$ infinitely supported (Definition \ref{def:infinite_support}) over some random variable $\RV{W}$, $(\prob{P}_\cdot,\RV{D},\RV{Y})$ is IO contractible over the same $\RV{W}$ if and only if the pairs $(\RV{D}_i, \RV{Y}_i)$ share conditionally independent and identical responses (Definition \ref{def:cii_rf}), given by the directing random conditional $\RV{H}$ (Definition \ref{def:dir_rand_cond}) and $(\prob{P}_\cdot,\RV{D},\RV{Y})$ is weakly data-independent.

\subsection{Statement of the representation theorem}\label{sec:reptheorem_statement}

\begin{theorem}[Representation of IO contractible models]\label{th:ciid_rep_kernel}
Suppose a sequential input-output model $(\prob{P}_\cdot,\RV{D},\RV{Y})$ with sample space $(\Omega,\sigalg{F})$ is given with $D$ countable and $\RV{D}$ infinitely supported over $\RV{W}$. Then the following are equivalent:
\begin{enumerate}
    \item There is some $\RV{W}$ such that $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible for all $\alpha$
    \item For all $i$, $\RV{Y}_i\CI (\RV{Y}_{\neq i},\RV{D}_{\neq i})|(\RV{H}, \RV{D}_i, \RV{C})$ and for all $i,j, \alpha$ $$\prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{D}_i}=\prob{P}_\alpha^{\RV{Y}_j|\RV{H}\RV{D}_j}$$
    \item There is some $\kernel{L}:H\times D\kto Y$ such that for all $\alpha$, $$\prob{P}_\alpha^{\RV{Y}|\RV{DH}}(\bigtimes_{i\in\mathbb{N}} A_{i}|d,h)= \prod_{i\in\mathbb{N}} \kernel{L}(A_i|d_i,h)$$
\end{enumerate}
\end{theorem}

\begin{proof}
(1)$\implies$(3):
We apply Lemma \ref{th:table_rep_kernel} followed by Lemma \ref{lem:ciid_yd} followed by Lemma \ref{lem:hw_interchange}.


(3)$\implies$ (2):
We verify that the required conditional independences hold assuming (3).

(2)$\implies$ (1):
We show that, assuming (2), then $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$ for all $\alpha$.

See Appendix \ref{sec:io_contract_models} for the full proof. Note that the proof uses string diagram notation explained in Appendix \ref{ssec:mken_diagrams}.
\end{proof}

Whenever we have an input-output model with conditionally independent and identical responses given some arbitrary $\RV{W}$, then we also have conditionally independent and identical responses given the directing random conditional $\RV{H}$.

\begin{corollary}\label{lem:ci_drc}
If a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ has independent and identical responses conditional on some variable $\RV{W}$ and $\RV{D}$ has infinite support over the same $\RV{W}$, then letting $\RV{H}$ be the directing random conditional with respect to inputs $\RV{D}$ and outputs $\RV{Y}$, it follows that for for all $i$, $\RV{Y}_i\CI \RV{W}|(\RV{D}_i,\RV{H},\RV{C})$ and for all $\alpha, i, j$, $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{H}}=\prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j\RV{H}}$.
\end{corollary}

\begin{proof}
We have by Theorem \ref{th:ciid_rep_kernel} that $\prob{P}_\alpha^{\RV{Y}|\RV{W}\RV{D}}$ is IO contractible over $\RV{W}$. The conclusion follows by applying Theorem \ref{th:ciid_rep_kernel} a second time.
\end{proof}

Building on Corollary \ref{lem:ci_drc}, Theorem \ref{th:infinite_condition_swaps} shows the assumption that the pairs $(\RV{D}_i,\RV{Y}_i)$ are related by conditionally independent and identical responses implies that, for the purposes of learning the response function $\RV{H}$, all infinite subsequences of $(\RV{D}_i,\RV{Y}_i)$ pairs with appropriate support are interchangeable. That is, suppose we have some infinite $A\subset \mathbb{N}$ for such that $(\prob{P}_\cdot,\RV{D}_A,\RV{Y}_A)$ is unimpeachably IO contractible over $*$ -- perhaps all pairs indexed by $A$ are derived from a carefully conducted experiment in precisely the conditions of interest to the decision maker and are therefore considered interchangeable in this strong sense. If we have some other infinite set $B\subset \mathbb{N}\setminus A$ of pairs derived from passive observation, then the assumption of conditionally independent and identical responses for the whole collection of pairs $(\RV{D}_i,\RV{Y}_i)_{i\in\mathbb{N}}$ implies that while we may not be able to swap individual pairs in $A$ with individual pairs in $B$, we must be able to swap the whole set $A$ for the whole set $B$ for the purposes of learning the response function $\RV{H}$.

\begin{theorem}\label{th:infinite_condition_swaps}
A data-independent sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with directing random conditional $\RV{H}$ and $\RV{D}$ infinitely supported over $\RV{H}$ features conditionally independent and identical response functions $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{H}}$ only if for any sets $A,B\subset \mathbb{N}$ such that $\RV{D}_A$ and $\RV{D}_B$ are also infinitely supported over $\RV{H}$ and any $i,j\in \mathbb{N}$ such that $i\not\in A$, $j\not\in B$, 
\begin{align}
\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{Y}_A,\RV{D}_A}=\prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j\RV{Y}_B\RV{D}_B}
\end{align}
If in addition each $\prob{P}_\alpha^{\RV{YD}}$ is dominated by some exchangeable $\prob{Q}_\alpha^{\RV{Y}\RV{D}}$, then the reverse implication also holds.
\end{theorem}

\begin{proof}
See Appendix \ref{sec:data_independent_proofs}.
\end{proof}

\subsection[Does IO contractibility help?]{Does IO contractibility help us understand identification?}\label{sec:symmetries_discussion}

One of the key contributions of De Finetti's representation theorem was to provide an alternative justification for the common modelling assumption that a sequence of variables were all distributed according to a shared but unknown ``true distribution''. De Finetti regarded the notion of an ``unknown true distribution'' as nonsensical, and through his representation theorem suggested that we could instead justify this structure by arguing that the experiment that produced the sequence of variables was, from the point of view of the analyst seeking to make predictions, invariant to reindexing the variables in the sequence.

Can IO contractibility help to justify common causal assumptions in a similar way? This question is less straightforward because IO contractibility is not such a straightforward symmetry. However, we think it does offer some insight into a common kind of causal assumption. Rather than lending justification to this assumption, the we think that it strengthens the case that this assumption is usually unreasonable.

The particular assumption we have in mind is, in the world of causal graphical models, the assumption that backdoor adjustment is possible and in the world of potential outcomes it is the assumption of \emph{conditional ignorability} \citep{rubin_causal_2005}. Both assumptions hold that, given a treatment $\RV{D}_i$, covariates $\RV{X}_i$ and an outcome $\RV{Y}_i$, there is an unknown but common conditional distribution of $\RV{Y}_i$ given $\RV{D}_i$ and $\RV{X}_i$ for all $i$, where $i$ ranges over passive observations as well as the consequences of actions. That is, we assume that the pairs $((\RV{D}_i,\RV{X}_i),\RV{Y}_i)$ share conditionally independent and identical responses. The key implication is Theorem \ref{th:infinite_condition_swaps}, which holds that, if the sequences of observations and consequences are both infinite, then for the purpose of learning the response function the problem is unchanged by swapping any subset of the indices corresponding to observations with any subset of those corresponding to consequences. That is, there is no difference between predicting the response function of the passive observations from an infinite sequence of passive observational data and predicting the response function of the consequences of the decision makers actions from the same sequence of passive observational data.

In practice, we propose that it would be very rare to have both of these datasets and treat them as interchangeable in this manner. Example \ref{ex:no_infinite_swapping} makes a similar point.

\begin{example}\label{ex:no_infinite_swapping}
Suppose an experiment is done which assigns some medical treatment $\RV{D}_i$ uniformly according to some random signal to patients for even $i$, and allows assignment by patient and doctor discretion for odd $i$. $\RV{Y}_i$ is a binary variable recording some health outcome of interest and $\RV{X}_i$ is some vector of covariates. The sequence $(\RV{D}_c,\RV{X}_c,\RV{Y}_c)$ is associated with the consequences of a decision maker's choices, where $c$ is some special character not in $\mathbb{N}$.

According to Theorem \ref{th:infinite_condition_swaps}, the assumption of conditionally independent and identical responses applied to $((\RV{D},\RV{X}),\RV{Y})$ implies
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_c|\RV{D}_c\RV{X}_c\RV{D}_{\text{odds}}\RV{X}_{\text{odds}}\RV{Y}_{\text{odds}}}&=\prob{P}_\alpha^{\RV{Y}_c|\RV{D}_c\RV{X}_c\RV{D}_{\text{evens}\setminus\{0\}}\RV{X}_{\text{evens}\setminus\{0\}}\RV{Y}_{\text{evens}\setminus\{0\}}}\\
    &=\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2\RV{X}_2\RV{D}_{\text{evens}}\RV{X}_{\text{evens}\setminus\{2\}}\RV{Y}_{\text{evens}\setminus\{2\}}}\\
    &=\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2\RV{X}_2\RV{D}_{\text{odds}}\RV{X}_{\text{odds}}\RV{Y}_{\text{odds}}}
\end{align}

That is, under this assumption, the following four problems are deemed identical:
\begin{itemize}
    \item Predicting the outcome of the decision maker's input from the experimental data
    \item Predicting the outcome of the decision maker's input from the observational data
    \item Predicting a held-out experimental outcome from the experimental data
    \item Predicting a held-out experimental outcome from the observational data
\end{itemize}
Any answer to one problem is, under this assumption, an answer for all of them. This is an assumption; we do not conclude this by comparing answers to these different problems and finding them to be the same, we simply assume it is so. The proposition that these problems are \emph{identical} is hard to swallow: it seems very unlikely, for example, if an analyst aiming to predict experimental results with access to the experimental data would be satisfied with their previous answer derived from the observational data.
\end{example}

In practice, when both experimental and observational data are available, they are \emph{not} assumed to be interchangeable in this sense -- in fact, the question of how well the observational data predicts experimental outputs is one of substantial interest \citet{eckles_bias_2021,gordon_comparison_2018,gordon_close_2022}.

