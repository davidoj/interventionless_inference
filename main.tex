%!TEX root = main.tex

% Due to file upload restrictions, this archive is missing the figures to render the string diagrams. They have been uploaded separately.
% They should be extracted to the figures folder in this directory

\documentclass{article}
\usepackage{arxiv}

\input{Custom_Settings}


 \author{ \href{https://orcid.org/0000-0003-3122-9767}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}David O. Johnston}\thanks{} \\
  Australian National University \\
  \texttt{davidoj@fastmail.com.au}\\
  %% examples of more authors
  \And
  Cheng Soon Ong \\
  Data61\\
  \And
  Robert C. Williamson \\
  Universität Tübingen \\
}

\renewcommand{\shorttitle}{Causal Inference Without Interventions}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}
  
\begin{document}
 

\title{Causal Inference Without Interventions}

\maketitle

\begin{abstract}
{Given a causal model and data, how does a decision maker infer consequences of their available actions? Typically they will assume a correspondence between their actions and structural intervention operations applied to the model. We argue that such a correspondence is typically not entailed by prior knowledge or by the given data. We investigate an alternative way to proceed. We distinguish \emph{decision models} as a general type of probabilistic model that appropriate for evaluating consequences given data from modelling assumptions like structural interventions that enable nontrivial inferences to be drawn. We investigate alternative assumptions to structural interventions. First, we examine the assumption that it is possible to identify sequences of paired ``input'' and ``output'' variables that share identical stochastic responses, and assumption we call \emph{conditionally independent and identical responses} (CIIR). Causal models that permit the inference of consequences typically satisfy this assumption for some sequence of pairs. We show that a generalisation of De Finetti's representation theorem applies to CIIR decision models and on the basis of this theorem, we provide a novel argument for the popular view that this assumption is usually unreasonable. Second, we consider the assumption of \emph{precedent} -- a version of the CIIR assumption where the ``input'' variables are unobserved. This may be interpreted as the assumption that anything the decision maker can do has been done before and its consequences observed, but the conditions that lead to those consequences were not observed. We show that this assumption together with the common structural assumption of \emph{inde[endent causal mechanisms} enables the inference of consequences without structural interventions.}
\end{abstract}
  \keywords{causal inference \and decision theory} 


\section{Introduction}

Judea Pearl's causal hierarchy distinguishes between three types of problems: prediction problems, intervention problems, and counterfactual problems \citep{pearl_book_2018}. Modelling an intervention problem requires a different kind of knowledge than that required for modelling predictions, and modelling a counterfactual problem requires a different kind of knowledge than that needed for modelling intervention problems.

While we think that Pearl's hierarchy offers an important insight into the differences between causal inference and classical statistics, we feel the terminology of interventions is confusing. In Pearl's theory, \emph{structural interventions} are used to model what he describes as intervention problems. Structural interventions are operations that transform a probability distribution according to a \emph{graphical causal model}. However, ``intervention problems'' refer to a broad class of problems that ask questions like ``what will happen if I do this? what will happen if I do something else instead?'' -- problems that we frequently encounter whether or not we make use of graphical causal models. This kind of problem often comes up we want to make a decision; given various options and some idea of the outcomes we would like to achieve, we want to know what the likely consequences of each option are.

As the terminology suggests, structural interventions and graphical causal models are often used as a tool for modelling the consequences of different options (that is, ``structural interventional models'' are used to model ``intervention problems'' broadly understood). If a decision maker wants to use structural interventions to model the consequences of different options, they must identify each of their options with a structural intervention. Even if they are able to learn many of the relevant causal relationships from a dataset -- using, for example, a method of causal discovery -- they usually will \emph{not} learn the correspondence between each of their options $\alpha$ and a particular structural intervention. This correspondence is usually taken to be prior knowledge. However, we contend that such a correspondence often goes beyond the prior knowledge a decision maker is likely to have.

The most common kind of structural intervention is known as a \emph{perfect} or \emph{hard} intervention. A perfect intervention is often referred to with the symbol $\mathrm{do}(\RV{X}=x)$. Given a probability distribution $\prob{P}$, a variable $\RV{X}$ and a causal graphical model $\mathcal{G}$ which, among other things, specifies a set of \emph{causal parents} $\mathrm{Pa}(\RV{X})$ of $\RV{X}$, the intervention $\mathrm{do}(\RV{X}=x)$ yields a new probability distribution $\prob{P}'$ such that the conditional probability $\prob{P}^{\prime \RV{X}|\mathrm{Pa}(\RV{X})}$ is everywhere $\delta_x$, while all other conditional distributions of a ``child'' conditional on its ``parents'' according to $\mathcal{G}$ match their counterparts in $\prob{P}$ \citep[Sec. 1.3.1]{pearl_causality:_2009}

Thus identifying a perfect intervention $\mathrm{do}(\RV{X}=x)$ with an option $\alpha$ embodies a collection of assumptions -- first, it embodies the assumption that selecting the option $\alpha$ will force future observations of the variable $\RV{X}$ to take the value $x$. This assumption may be justified where a decision maker knows at the outset how to deterministically control $\RV{X}$. However, the identification of options with perfect interventions also embodies the assumption that selecting the option $\alpha$ will leave all ``parental conditionals'' with respect to $\mathcal{G}$ unchanged with the exception of $\prob{P}^{\prime \RV{X}|\mathrm{Pa}(\RV{X})}$. What kind of knowledge could a decision maker have that justifies these further assumptions?

One possibility is that the decision maker also knows that all of these conditionals will remain unchanged. For example, if the decision maker is a programmer working on a program represented by the graph $\mathcal{G}$ and is reasoning about changing a particular function to output a constant. However, this is not a typical situation where someone is interested in causal inference or causal discovery.

Alternatively, the decision maker might simply assume that their options correspond to perfect interventions (or some other default transformation) on on some unknown causal graph, and attempt to determine the correct graph via a combination of prior knowledge and causal discovery. The notion of a default correspondence between options and interventions is compatible with the common idea that the causal graph is all you need to answer key interventional questions. Success in causal discovery is typically measured by the structural intervention distance \citep{peters_structural_2015} or the structural hamming distance, neither of which have any dependence on the options under consideration \citep{scherrer_learning_2022, toth_active_2022, brouillard_differentiable_2020, ngGraphAutoencoderApproach2019, forre_constraint-based_2018, zhengDAGsNOTEARS2018, chickering_optimal_2003, spirtes_causation_1993}. The idea that the relationships captured by a causal graph are independent of the options under consideration is also defended in \citet{pearl_does_2018}. 

The justification for a default choice of intervention is hard to see. One could imagine an extensive survey of decision problems concluding that a particular kind of intervention is nearly always suitable, but we are not aware of any such undertaking. Furthermore, it does not appear to be particularly difficult to come up with common decision problems where perfect interventions care inappropriate. Hernán and Taubman consider the example of different options that are known a priori to affect a person's body mass index, including diet plans, gastric bypass surgery and limb removal \citep{hernan_does_2008,noauthor_does_2016}. These will all plausibly affect an individual's risk of death differently, and so they cannot all be modeled by the same kind of intervention on body mass index. Further, it seems to us (as well as other authors in this exchange \citep{pearl_does_2018,hernanInvitedCommentaryCausal2009,shahar_association_2009}) that none of these options stand out as a strong candidate to be identified with a perfect intervention on body mass index.

One possible response is to model consequences via perfect interventions on diet, gastric surgery or limb count. This approach raises similar concerns about interventions on the new variables (there's more than one way to raise the probability that someone adopts a diet). It also seems to undermine the project of learning useful causal structures from convenient datasets. In particular, it suggests that causal structures are of little use unless some special ``intervenable'' variables can be included in these structures.

So far our remarks have concerned perfect interventions, but there is a diverse array of structural interventions that can be found in the literature. A non-exhaustive review turns up perfect interventions or ``hard'' interventions \citep[ch. ~1]{pearl_causality:_2009,hauser_characterization_2012}, soft interventions \citep{correa_calculus_2020,eberhardt_interventions_2007}, general or fat-hand interventions \citep{eberhardt_interventions_2007,yang_characterizing_2018,glymour_evaluating_2017} and general interventions with unknown targets \citep{brouillard_differentiable_2020}. We could consider using any of these families of interventions to address our decision maker's dilemma: how to relate their actions and their causal graph to distributions over consequences. The richer family of interventions can address the problem of nonuniqueness above -- some actions that affect the same target may be modeled as different types of intervention. While generalised interventions address the problem that it is \emph{impossible} to represent all actions as perfect interventions, they still embody assumptions that go beyond the causal graph and , we note that the references cited for generalised interventions do not use them for the purposes of evaluating prospective actions However, the problem still seems to be a difficult one: our decision maker who may, in general, be quite ignorant about many of the details of the relevant causal graph, must say something about which action corresponds to which kind of intervention. While this problem may be solvable, we investigate the prospects for avoiding it altogether.

To clarify our claim: we doubt that models for data-driven decision making should rely on structural interventions as a primitive notion. We do not doubt that structural interventional models are practically useful in a wide range of cases. In some cases, interventions describe very precisely the outcomes that should be expected -- for example, consider the consequences of altering a single function to output a constant value in a piece of computer software. In many more cases the idea of structural interventions are useful enough even where our understanding of why they provide a good model of actions is less precise. For example, causal graphical models have been widely used for understanding interactions between genes \citep{badshaLearningCausalBiological2019}. Any approch to building models for data-driven decision making that avoids structural interventions as primitives probably needs to include structural interventions as a common and important special case.

We must make \emph{some} assumptions to licence inferences about the consequences of our actions. It is not enough to assume that observations exhibit some regularity (for example, being independent and identically distributed) and that we have prior knowledge about how different actions control some variables. We want to leverage the observations to draw conclusions about how actions will affect variables which we do not know how to control at the outset. The assumption of invariant parental conditional distributions that are a part of structural interventional models allow us to make these inferences. If a decision maker wants to learn from some observed data so as to make a better decision, then these assumptions tell them what the observational data and the consequences of their decisions will have in common: the parental conditional distributions.

In this work, we ask what other kinds of assumptions might capture the idea that the consequences of our actions are somehow related to our observations. We draw inspiration from the classic statistical assumption of \emph{exchangeability}, which expresses the idea that observations taken at different points in time are (in a particular way) just like one another. While it does capture a sense in which observations after taking an action are related to observations made before taking an action, it is not a useful assumption for deceision making. In decision problems, future events are affected by the choices made by the decision maker and therefore are not just like observations taken prior to making a decision.

We investigate two modified assumptions motivated by the idea of exchangeability. First, we introduce the idea of \emph{input-output contractibility} (IO contractibility), which can be viewed as a generalisation of exchangeability to decision models. In particular, we show a theorem analogous to De Finetti's famous representation \citep{de_finetti_foresight_1992} theorem holds; IO contractibility is equivalent to the assumption that there is a shared but unknown input-output map for both the observations and the consequences of a decision maker's choices (Theorem \ref{th:ciid_rep_kernel}). Unlike exchangeability, however, IO contractibility is not an appealing assumption in many data-driven decision problems.

We subsequently explore the related idea of \emph{precedent}, which holds that every action a decision maker can take has, in a sense, been done before and its consequences observed, though the deicision maker generally doesn't know \emph{when} it has been done before.  We show that with the assumption of precedent together with \emph{conditional absolute continuity}, an observed conditional independence may imply that an observed conditional distribution is unchanged by any action a decision maker might take (Theorem \ref{th:latent_to_observable}). Conditional absolute continuity is the assumption that, roughly speaking, certain conditional distributions are not ``precisely coordinated''.

Standard interpretations of structural models hold that the right causal structure implies conditional absolute continuity -- in particular, that conditional distributions corresponding to cause-effect relationships are not precisely coordinated. This feature of structural causal models is considered to hold \emph{in addition} to the requirement that a decision maker's actions correspond some how to structural interventions (which we have argued above is a problematic assumption). We show that this latter assumption is not necessary to derive consequences of actions from causal models. Instead of an assumed action-intervention correspondence, we can make the assumption of precedent, and given the right causal structure and observed conditional independence, this is enough to infer consequences of actions.

Conditional absolute continuity is closely related to the informal principle of \emph{independent causal mechanisms}. Both hold that we should consider it unlikely for ``causal mechanisms'' (that is, distributions of effects given their causes, as identified by structural models) to be precisely coordinated. This principle has been suggested as an alternative foundation for causal discovery \citep{lemeire_replacing_2013}.

Section \ref{sec:tech_prereq} introduces the formalism of decision models. These differ from probability models in that they are a map from a set of options to distributions over consequences. We make use of the notion of \emph{extended conditional independence} introduced by \citet{constantinou_extended_2017}, which is a notion of conditional independence relevant to decision models. Section \ref{sec:evaluating_decisions} introduces the idea of shared conditionally independent and identical responses, and shows that this is equivalent to the assumption of input-output in Theorem \ref{th:ciid_rep_kernel}. Section \ref{sec:precedent} explains the assumption of precedent and proves Theorem \ref{th:latent_to_observable} and discusses the interpretation of this assumption and its connection to structural causal models.

\subsection{Previous work on symmetries in causal inference}\label{sec:prev_work}

Our approach starts with the assumption that we are trying to model a decision problem, and not address any other kind of problem that comes under the umbrella of causal inference. This assumption motivates the formalism of ``decision models'' that we use to investigate the questions raised here. The broad idea of starting with the options available to a decision maker rather than starting with some foundational notion of causation is often called the \emph{decision theoretic approach to causal inference} \citep{heckerman_decision-theoretic_1995,dawid_decision-theoretic_2012,dawid_decision-theoretic_2020}. \citet{lattimore_causal_2019,lattimore_replacing_2019} also document an approach to causal modelling that demands explicit consideration of the set of interventions, and is arguably an example of the decision theoretic approach.

\citet{lindley_role_1981} discussed sequences of exchangeable observations along with ``one more observation''. Lindley mentioned the application of this model to questions of causation, but did not explore this deeply due to the perceived difficulty of finding a satisfactory definition of causation. \citet{rubin_causal_2005, imbens_causal_2015} made use of the assumption of models with exchangeable potential outcomes to prove several identification results. \citet{saarela_role_2020}, used graphical causal models to propose \emph{conditional exchangeability}, defined as the exchangeability of the non-intervened causal parents of a target variable under intervention on its remaining parents. Sareela et. al. suggested that this could be interpreted as a symmetry of an experiment involving administering treatments to patients with respect to exchanging the patients in the experiment. \citet{hernan_estimating_2006,hernan_beyond_2012,greenland_identifiability_1986,banerjee_chapter_2017,dawid_decision-theoretic_2020} all discuss similar experimental symmetries. These symmetries are reminiscent of \emph{exchange commutativity} discussed here. They're not identical, however -- exchange commutativity can be justified by the equivalence of certain prediction problems that arise from a single experiment, instead of an equivalence of different experiments that arise from, for example, interchanging experimental subjects.

A different kind of regularity of causal models is given by the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020} and the stable unit treatment value assumption (SUTVA) in \citep{rubin_causal_2005}. This regularity is similar to the condition of \emph{locality} introduced here.

Theorem \ref{th:latent_to_observable} was influenced by the idea of \emph{causal inference by invariant prediction} \citep{peters_causal_2016}. While both the assumptions and the conclusions drawn in that work differ from the assumptions and conclusion of Theorem \ref{th:latent_to_observable}, both proceed from an idea that can be roughly described as ``things I can do have been done before'' and both look for variable pairs $\RV{X}$ and $\RV{Y}$ such that the distribution of $\RV{Y}$ given $\RV{X}$ doesn't change after actions are taken. Finally, the variable described in that work as ``the environment'' is similar to the variable $\RV{Z}$ in Theorem \ref{th:latent_to_observable}. A key difference is that their result is proved for certain classes of structural interventions as the ``things that can be done'', while Theorem \ref{th:latent_to_observable} does not depend on classes of structural interventions.

Finally, \citet{guoCausalFinettiIdentification2022} have recently generalised De Finetti's theorem to causal graphs in a different manner to the present work and analysed how causal structure may be inferred from independences in exchangeable models.

\subsection{Outline}

Section \ref{sec:tech_prereq} outlines our mathematical framework and defines key terms. Section \ref{sec:standard_prob} introduces probability theory that may be familiar to many readers, while Section \ref{sec:probability_sets} introduces decision models, and extension of standard probability models and the basic kind of model we consider for the rest of this work. We also describe \emph{extended conditional independence}, an extension of standard conditional independence suitable for decision models. Extended conditional independence is not original to this work, and was introduced by \citet{constantinou_extended_2017}.

Section \ref{sec:ccontracibility} first explains decision models with \emph{conditionally independent and identical responses}, an analogue of the common assumption of conditionally independent and identicallly distributed variables. We then introduce and explains Theorem \ref{th:ciid_rep_kernel}, a ``decision model analogue'' for De Finetti's represention theorem, and finally argue, using Theorem \ref{th:ciid_rep_kernel}, that the assumption of conditionally independent and identical responses is often unreasonable.

Section \ref{sec:precedent}, in response to the need for a weaker assumption than conditionally independent and identical responses, introduces the idea of precedent via an example, and then introduces Theorem \ref{th:latent_to_observable} which establishes that under some additional conditions the assumption of precedent can imply conditionally independent and identical responses. One of these additional requirements is a conditional independence, which is a common requirement to draw causal conclusions from observational data. The other condition is a stronger version of precedent known as \emph{diverse precedent}. The interpretation of this assumption is not obvious. We postulate that it may be derivable from assumptions of causal direction along with the principle of independent causal mechanisms, but leave this as an open question.

\input{technical}
\input{contractibility}
\input{precedent}

\bibliographystyle{plainnat}

\bibliography{library}

\appendix

\include{appendix}

\end{document}