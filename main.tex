%!TEX root = main.tex

% Due to file upload restrictions, this archive is missing the figures to render the string diagrams. They have been uploaded separately.
% They should be extracted to the figures folder in this directory

\documentclass{article}
\usepackage{arxiv}

\input{Custom_Settings}


 \author{ \href{https://orcid.org/0000-0003-3122-9767}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}David O. Johnston}\thanks{} \\
  Australian National University \\
  \texttt{davidoj@fastmail.com.au}\\
  %% examples of more authors
  \And
  Cheng Soon Ong \\
  Data61\\
  \And
  Robert C. Williamson \\
  Universität Tübingen \\
}

\renewcommand{\shorttitle}{Causal Inference Without Interventions}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Causal Inference Without Interventions},
pdfauthor={David O. Johnston, Cheng Soon Ong, Robert C. Williamson},
pdfkeywords={causal inference, decision theory},
}
  
\begin{document}
 

\title{Causal Inference Without Interventions}

\maketitle

\begin{abstract}
{If we have data and want to evaluate the consequences of an action, we can use a causal model. In this model, actions are represented by structural interventions. However, for many variables the connection between actions and structural interventions is not obvious. This raises the question: if we learn a causal model but do not know how actions correspond to interventions, have we learned anything useful? We show, eventually, that the answer may be ``yes''. We begin with \emph{decision models}, which map options to distributions over outcomes but are otherwise agnostic to causality. We first analyse inference under the assumption of \emph{conditionally independent and identical responses} (CIIR), an assumption of a consistent relationship between pairs of input and output variables. For CIIR models, we prove that arbitrary sequences of input-output pairs can be exchanged given sufficient data. On this basis, we argue for a negative result: it is usually unreasonable to assume causal effects are identified in observational data (this is a common view, but our argument is novel). However, we also show a positive result: \emph{precedent} is a version of the CIIR assumption in which inputs are unobserved. We show that precedent plus the right causal structure and conditional independence implies pairs of observed variables also satisfy CIIR. Here. causal structures encode the assumption of independent causal mechanisms but not intervention operations. That is, even without interventions, causal structure and precedent can be enough to learn about the consequences of your actions.}
\end{abstract}
  \keywords{causal inference \and decision theory} 


\section{Introduction}

Sometimes we want to make decisions supported by data. Structural causal models are a standard framework for addressing this kind of problem. Roughly speaking, there are two different ways to use the structural modelling framework to help make decisions: in some situations, we are confident in our ability to construct the correct structural model from prior information, and the model helps us draw valid deductions from this prior information. On the other hand, in some situations we have little prior information about the correct structural model, and we might attempt \emph{causal discovery} in order to learn it from the given data. We can then use the learned model to draw valid inferences as before.

It is a well-known difficulty of causal inference that the available data may not be sufficient to identify the causal effects of interest. A further difficulty is that the options a decision maker has available often do not have a simple correspondence to interventions on the learned causal graph. This means that, even if a structural model is known, additional knowledge is needed to determine how the consequences of actions should be modeled. This may generally be particularly problematic in settings that call for causal discovery, because in such situations we do not have detailed prior knoweldge about causal relationships.

Consider, for example, an author who wants to know what genre to pick for their next book in order to maximise sales -- science fiction or romance. Suppose that, using a large dataset from a bookseller, they are able to learn a structural model. The exact nature of the model is not critical for the example, but for concreteness we will suppose it contains three variables: sales $\RV{S}$, genre $\RV{G}$ (as judged by the bookseller) and ``covariates'' $\RV{V}$, which is actually a pair of variables: a statistic based on the author's historical sales record, and a statistic based on recent trends in genre sales.

\begin{align}
  \tikzfig{dag_book_model}\label{eq:dag_book_model}
\end{align}

The theory of perfect interventions tells us that, given a distribution $\prob{P}(\RV{S},\RV{G},\RV{V})$ estimated from the data, an intervention on genre will yield the intervened distribution

\begin{align}
  \prob{P}(\RV{S},\RV{V}|\mathrm{do}(\RV{G}=x)) = \prob{P}(\RV{V})\prob{P}(\RV{S}|\RV{V},\RV{G}=x)\label{eq:perfect_intervention}
\end{align}

Suppose that the conditional $\prob{P}(\RV{S}|\RV{V},\RV{G})$ tells us that at any level of author skill, given observed recent sales trends, romance novels are expected to outsell science fiction novels. This seems to suggest that the author is well-advised to write a romance novel.

However, there is a difference between deciding to write a romance novel and actually writing one. Having made the decision, one is not guaranteed to end up having written a romance novel (at least, according to the bookseller) -- or even having written a novel at all. Furthermore, there are an enormous number of different ways to end up with a book classed as ``romance''. The author could, for example, write any novel at all and press booksellers to label it ``romance''. Even if they do write a book that is honestly classed as ``romance'', there are an enormous number of different books that qualify, and many of these different books are likely to have different sales prospects.

Inspection of the model \eqref{eq:dag_book_model} does not tell us which, if any, of these options is a ``canonical intervention on genre''. This problem is somewhat reminiscent of the problem of ambiguous manipulations raised by \citet{spirtes_causal_2004}. Spirtes and Scheines note that when a variable is composed of multiple variables with clear intervention semantics, the composite variable often fails to have clear intervention semantics of its own. \citet[Ch. ~11]{pearl_causality:_2009} states ``there is no way a model can predict the effect of an action unless one specifies which variables are affected by the action and how.''

We could propose is that causal models are not the right tool for judging the consequences of ``coarse'' actions like choosing the genre of a book to write -- perhaps they are meant to give more precise answers to more precise questions. However, on the face of it, it seems reasonable to ask: on the basis of given data, what genre should I aim to write? Perhaps our author could do better by deliberating over more detailed plans, but picking the genre suggested by \eqref{eq:dag_book_model} seems better than picking randomly.

There are many other decision problems for which the relationship between actions and interventions is ambiguous. A notable example is the idea of an intervention on \emph{body mass index} examined by \citep{hernan_does_2008,noauthor_does_2016}. Hernán and Taubman consider the example of different options that are known a priori to affect a person's body mass index, including diet plans, gastric bypass surgery and limb removal. As with actions affecting genre, none of these options seem to be viable candidates for a ``canonical intervention on body mass index'', and opinion shared by other authors \citep{pearl_does_2018,hernanInvitedCommentaryCausal2009,shahar_association_2009}).

In this paper, we investigate an approach to modelling decision problems that takes supporting decisions, rather than representing causal relationships, to be the primary objective. \emph{Decision models} represent actions we can take and consequences we want to evaluate, while representing causal relationships is entirely optional. Such models have been studied previously by numerous authors, including \citet{heckerman_decision-theoretic_1995,dawid_decision-theoretic_2012,dawid_decision-theoretic_2020,lattimore_causal_2019,lattimore_replacing_2019}. An advantage of decision models is that, by construction, it is clear how actions correspond to features of the model. A disadvantage is that, unlike causal models, decision models do not come equipped with inference rules ``by default''. Thus the main objective of this paper is to explore inference rules in decision models.

Independent and identically distributed (IID) sequences of variables are a foundational concept in statistics. The first inference rule we introduce is a generalisation of the IID assumption. Variable sequences with independent and identical responses (IIR) feature pairs of ``input'' and ``output'' variables related by identical stochastic response functions. For IIR decision models with unknown response functions, we prove a generalisation of De Finetti's representation theorem \citep{de_finetti_foresight_1992}. We show that an IIR decision model with an unknown response function is equivalent to an \emph{input-output contractible} decision model (Theorem \ref{th:ciid_rep_kernel}). IO-contractibility is a somewhat complex symmetry of decision models which implies (among other things) the interchangeability of sufficiently large samples of experimental and observational data.

IIR sequences, however, is not a promising inference rule in general. We argue that sufficiently large samples of experimental and observational data are usually not interchangeable, and thus it is usually unreasonable to use IIR decision models. This is not a novel view; it is analogous to the standard view that causal effects are typically not known to be identified in observational data. However, our argument for this conclusion is new, which shows that the decision model framework can at least offer a different perspective on familiar problems.

We then turn to a more promising inference rule. This inference rule has a number of conditions which are somewhat complex, and will be explained in more detail in Section \ref{sec:precedent}. Briefly, the first condition is \emph{precedent}. In the context of our book writing example, this is the assumption that, given either of the available options (write a science fiction or romance book), the distribuiton over sales is given by some unknown reweighting of the observations. The next condition is an observed conditional independence -- in our example, this could be the observation that book sales are independent of the author's identity given the genre and the identified covariates. The final condition is \emph{absolute continuity of conditionals}. This requirement is not easy to explain without the formalism we introduce, and so we will save the explanation for the relevant section. This condition is implied by certain causal structures along with the assumption of \emph{independent causal mechanisms}, such as the following:

\begin{align}
  \tikzfig{dag_book_model_augmented}\label{eq:dag_book_model2}
\end{align}

These three conditions, together, justify assuming that the consequences of (successfully) writing a romance novel are the interventional consequences as given by the original model \eqref{eq:dag_book_model} (Theorem \ref{th:latent_to_observable}). Thus, rather than needing to assume a correspondence between actions and interventions -- which we've argued can be problematic -- we can derive it from the previously mentioned conditions.

The assumption of independent causal mechanisms underpins the assumption of \emph{faithfulness} that facilitates conditional independence based causal discovery algorithms \citep{meek_strong_1995}, as well as many alternative approaches to causal discovery \citep{lemeire_replacing_2013}. Theorem \ref{th:latent_to_observable} also suggests that it could also underpin the interventional interpretation of structural causal models, without needing to introduce interventions as an additional assumption (though this is only a preliminary suggestion, and it may fail to play out under further investigation).

\subsection{Connections to previous work in causal inference}\label{sec:prev_work}

Our approach starts with the assumption that we are trying to model options and consequences, and we do not demand that our models capture any other notion of causation. This assumption motivates the formalism of ``decision models''. This approach is in the tradition of the the \emph{decision theoretic approach to causal inference} that has been formalised in slightly different ways by \citet{heckerman_decision-theoretic_1995} and \citet{dawid_decision-theoretic_2012,dawid_decision-theoretic_2020}. While \citet{lattimore_causal_2019,lattimore_replacing_2019} do not explicitly call their approach ``decision theoretic'', it is also similar to our approach in that options are explicitly incorporated into the model.

\citet{lindley_role_1981} discussed of sequences of exchangeable observations along with ``one more (possibly non-exchangeable) observation''. This construction is very similar to our ``see-do models'' (Definition \ref{def:seedo}). Lindley mentioned the application of this model to questions of causation, but did not explore this deeply due to the perceived difficulty of finding a satisfactory definition of causation. 

There have been a number of works on symmetries in causal inference reminiscent of our work on input-output contractibility. \citet{rubin_causal_2005, imbens_causal_2015} made use of the assumption of models with exchangeable potential outcomes to prove several identification results. \citet{saarela_role_2020}, used graphical causal models to propose \emph{conditional exchangeability}, defined as the exchangeability of the non-intervened causal parents of a target variable under intervention on its remaining parents. Sareela et. al. suggested that this could be interpreted as a symmetry of an experiment involving administering treatments to patients with respect to exchanging the patients in the experiment. \citet{hernan_estimating_2006,hernan_beyond_2012,greenland_identifiability_1986,banerjee_chapter_2017,dawid_decision-theoretic_2020} all discuss similar experimental symmetries. A key difference between all of these causal symmetries and input-output contractibility is that these are all counterfactual symmetries -- they say that, had the experiment been performed differently (say, if different treatments had been administered to different patients), the same model would be used to analyse it. Input-output contractibility, on the other hand, is a data symmetry -- it holds that there are certain transformations that do not affect the choice of appropriate model. Our work on input-output contractibility is also distinguished by the fact that we prove the equivalence of input-output contractible decision models and decision models with conditionally independent and identical responses, which is required in any case where any conditionals that arise ``as a consequence of my actions'' are thought to be identical to conditionals in previously observed data.

A different kind of regularity of causal models is given by the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020} and the stable unit treatment value assumption (SUTVA) in \citep{rubin_causal_2005}. This regularity is similar to the condition of \emph{locality}, a subassumption of input-output contractibility.

Theorem \ref{th:latent_to_observable} was inspired by \emph{causal inference by invariant prediction} \citep{peters_causal_2016}. While both the assumptions and the conclusions drawn in that work differ from the assumptions and conclusion of Theorem \ref{th:latent_to_observable}, both look for variable pairs $\RV{X}$ and $\RV{Y}$ such that the distribution of $\RV{Y}$ given $\RV{X}$ doesn't change when actions are taken. Unlike Peters et. al., our result does not make use of structural interventions, and the connection to the principle of independent causal mechanisms is original to this work.

Finally, \citet{guoCausalFinettiIdentification2022} have recently generalised De Finetti's theorem to causal graphs in a different manner to the present work and analysed how causal structure may be inferred from independences in exchangeable models.

\subsection{Outline}

Section \ref{sec:tech_prereq} outlines our mathematical framework and provides a brief reference on notation. Section \ref{sec:ccontracibility} introduces decision models with \emph{conditionally independent and identical responses}, a generalisation of conditionally independent and identicallly distributed variables. We then introduce and explains Theorem \ref{th:ciid_rep_kernel}, a ``decision model analogue'' for De Finetti's represention theorem, and finally argue, on the basis of this theorem, that the assumption of conditionally independent and identical responses is often unreasonable.

Section \ref{sec:precedent} looks at a more promising inference rule. It introduces the notion of precedent and then proves Theorem \ref{th:latent_to_observable}, which establishes that precedent along with some additional conditions implies conditionally independent and identical responses. We then examine these additional conditions in more detail, and show that they are supported by structural assumptions, if those assumptions are interpreted as expressing the independence of certain conditional probabilities.

\input{technical}
\input{contractibility}
\input{precedent}

\bibliographystyle{plainnat}

\bibliography{library}

\appendix

\include{appendix}

\end{document}