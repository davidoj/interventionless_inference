%!TEX root = main.tex

\section{Technical Prerequisites}\label{sec:tech_prereq}

Our approach to causal inference is based on probability theory. This section gathers some necessary technical definitions, and is included for reference. A reader who wishes to follow the arguments of the paper may skip to Section \ref{sec:evaluating_decisions} and refer back to this section as required.

Section \ref{sec:standard_prob} introduces the notation used in this paper. Because decision models are stochastic functions rather than probability measures (Section \ref{sec:probability_sets}), we make use a generalisation of conditional independence called \emph{extended conditional independence}, explained in Section \ref{sec:eci} -- the content of these sections may be less familiar.


\subsection{Probability Notation}\label{sec:standard_prob}

We refer a reader to \citet{cinlar_probability_2011} chapters 1, 2 and 4 for an introduction to the probability theory we use use in this paper. Here we offer a brief overview of our notation.

We denote a measurable space $(A, \sigalg{A})$. Given a collection $U$ of subsets of $A$, $\sigma(U)$ is the smallest $\sigma$-algebra containing $U$.

For any $A$, $\{\emptyset,A\}$ is a $\sigma$-algebra. For countable $A$, the power set $\mathscr{P}(A)$ is known as the discrete $\sigma$-algebra.

We write random variables $\RV{X}:A\to X$; the variable and its codomain share names but are written with different fonts. Given a probability measure $\prob{P}$ on $(A,\sigalg{A})$, $\prob{P}^{\RV{X}}$ is the marginal distribution of $\RV{X}$ and $\prob{P}^{\RV{X}|\RV{Y}}$ is the conditional distribution of $\RV{X}$ given $\RV{Y}$.

A sequence of random variables $(\RV{X},\RV{Y})$ is itself a random variable $\omega\mapsto (\RV{X}(\omega), \RV{Y}(\omega))$. We denote by $*$ a random variable $*:A\mapsto \{*\}$ where $(\{*\}, \{\emptyset, \{*\}\})$ is equipped with the \emph{indiscrete} or ``trivial'' $\sigma$-algebra.

We denote by $\Delta(A)$ the set of all probability measures on $(A,\sigalg{A})$.

We use the Iverson bracket $\llbracket \mathrm{condition} \rrbracket$ for the function that evaluates to $1$ if \emph{condition} is true and $0$ if it is false. The Dirac measure $\delta_x \in \Delta(X)$ is the probability measure for which $\delta_x(A) = \llbracket x\in A \rrbracket$.

\section{Decision Models}

We are interested in modelling decision making rather than prediction. The key difference is that different decisions can lead to different outcomes. This is not the case for prediction problems, where the outcome is unaffected by the prediction offered.

Decisions differ from ordinary random variables in that a decision maker does not require a probability distribution over their options. A decision maker requires a model that accepts any proposed option and offers a forecast of its consequences, and on the basis of this model they make their decision. A probability distribution over options does not contribute to this decision making process, and so it is not required in a decision model (in fact, it is generally accepted that -- whatever their merits -- probability distributions over options should not contribute to decision making \citet{liu_ramsey_2020}).

Instead of a probability distribution which offers unconditional forecasts of outcomes, a decision maker requires a function that maps their options to outcomes. We model this with a \emph{Markov kernel}, a function that maps options to probability distributions.

\begin{definition}[Markov kernel]\label{def:markov_kern}
Given measurable spaces $(E,\sigalg{E})$ and $(F,\sigalg{F})$, a \emph{Markov kernel} or \emph{stochastic function} is a map $\kernel{M}:E\times\sigalg{F}\to [0,1]$ such that
\begin{itemize}
	\item The map $\kernel{M}(A|\cdot):x\mapsto \kernel{M}(A|x)$ is $\sigalg{E}$-measurable for all $A\in \sigalg{F}$
	\item The map $\kernel{M}(\cdot|x):A\mapsto \kernel{M}(A|x)$ is a probability measure on $(F,\sigalg{F})$ for all $x\in E$
\end{itemize}
\end{definition}

We use an alternative notation for the signature of a Markov kernel to stress the fact that we can consider it a map from a measurable set to a set of probability distributions.

\begin{notation}[Signature of a Markov kernel]
Given measurable spaces $(E,\sigalg{E})$ and $(F,\sigalg{F})$ and a Markov kernel $\kernel{M}:E\times\sigalg{F}\to [0,1]$, we write $\kernel{M}:E\kto F$, which we read as ``$\kernel{M}$ maps from $E$ to probability measures on $F$''.
\end{notation}

A \emph{decision model} is a generalisation of a \emph{probability space}. A decision model combines an \emph{option set}, a sample space and a Markov kernel that maps from the option set to probability measures on the sample space.

\begin{definition}[Decision model]\label{def:dec_model}
A decision model is a triple $(\prob{P}_\cdot, (\Omega,\sigalg{F}), (C,\sigalg{C}))$ where $\prob{P}\cdot:C\kto \Omega$ is a Markov kernel, $(\Omega,\sigalg{F})$ is the sample space and $(C,\sigalg{C})$ is the set of choices (or options).
\end{definition}

As in standard probability spaces, we take random variables to be measurable functions on the sample space.

\begin{definition}[Random variable]\label{def:variable}
Given a decision model $(\prob{P}_\cdot, (\Omega,\sigalg{F}), (C,\sigalg{C}))$, an \emph{$X$-valued random variable} is a measurable function $\RV{X}:(\Omega,\sigalg{F})\to (X,\sigalg{X})$.
\end{definition}

\subsection{Conditional distributions and conditional independence in decision models}

Decision models differ from regular probability models in that we always need a choice $\alpha$ from the set $C$. There is in general no unconditional ``$\prob{P}$'' nor any distribution over choices ``$\prob{P}^{\RV{C}}$''. This is due to the fact that we noted above -- a decision maker does not make use of a distribution over $\RV{C}$ in the course of their deliberation, and this is reflected by the absence of any distribution not conditioned on a choice.

If we have two random variables $\RV{X}$ and $\RV{Y}$, for each $\alpha\in C$ we have a conditional distribution $\prob{P}^{\RV{Y}|\RV{X}}_\alpha$.

The only notion of conditional independence we require is conditional independence for each $\alpha\in C$. We say $\RV{X}\CI \RV{Y} | (\RV{Z}, \RV{C})$ -- read ``$\RV{X}$ is independent of $\RV{Y}$ given $\RV{Z}$ and $\RV{C}$'' if, for each $\alpha\in C$, $\RV{X} \CI_{\prob{P}_\alpha} \RV{Y} | \RV{Z}$ -- that is, relative to $\prob{P}_\alpha$, $\RV{X}$ is independent of $\RV{Y}$ given $\RV{Z}$.

There is an extended notion of conditional independence for decision models given by \citet{constantinou_extended_2017}, which is substantially more general than the notion we use.

Our notion of conditional independence satisfies the standard properties:

\begin{enumerate}
    \item Symmetry: $\RV{X}\CI \RV{Y}|(\RV{Z}, \RV{C})$ iff $\RV{Y}\CI \RV{X}|(\RV{Z},\RV{C})$
    \item $\RV{X}\CI \RV{Y}| (\RV{Y}, \RV{C})$
    \item Decomposition: $\RV{X}\CI (\RV{Z}, \RV{Y})|(\RV{W}, \RV{C})$ implies $\RV{X}\CI_{\prob{P}_{\cdot}}^e\RV{Z}|(\RV{W}, \RV{C})$ and $\RV{X}\CI_{\prob{P}_{\cdot}}^e\RV{Y}|(\RV{W}, \RV{C})$
    \item Weak union:
    \begin{enumerate}
     	\item $\RV{X}\CI (\RV{Y}, \RV{Z})|(\RV{W}, \RV{C})$ implies $\RV{X}\CI (\RV{Y},\phi)|(\RV{Z},\RV{W}, \RV{C})$
     \end{enumerate} 
    \item Contraction: $\RV{X}\CI \RV{Z}|(\RV{W}, \RV{C})$ and $\RV{X}\CI \RV{Y}|(\RV{Z},\RV{W}, \RV{C})$ implies $\RV{X}\CI(\RV{Y},\RV{Z})|(\RV{W}, \RV{C})$
\end{enumerate} 

In a decision model, we say that two random variables are almost surely equal if they are almost surely equal for every $\alpha\in C$. That is, given $\RV{X}$, $\RV{Y}$

\begin{align}
	\RV{X} &\overset{C}{\cong} \RV{Y}\\
	&\iff\\
	\prob{P}_\alpha(\RV{X}\neq \RV{Y}) &= 0 &\forall \alpha\in C
\end{align}

We say that two random variables are almost surely equal on a particular $\alpha'\in C$ if they are almost surely equal for $\alpha'$ but not necessarily for any other element of $O$:

\begin{align}
	\RV{X} &\overset{\alpha'}{\cong} \RV{Y}\\
	&\iff\\
	\prob{P}_{\alpha'}(\RV{X}\neq \RV{Y}) &= 0
\end{align}


\subsection{Directed graphs}\label{sec:d_graphs}


\begin{definition}[Directed graph]
A directed acyclic graph $\mathcal{G}$ is a set of nodes $\mathcal{V}$ and a set of edges $\mathcal{E}$. Each edge is an ordered pair of nodes $(V_i,V_j)\in \mathcal{V}^2$, with $V_i$ the source and $V_j$ the destination. An acyclic graph must have no directed path that begins and ends at $V_i$ for any $V_i\in\mathcal{V}$.
\end{definition}

\begin{definition}[Directed path]
Given a graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, a directed path is a sequence of edges $((V^1_{k},V^2_{k}))_{k\in [n]}$ from $\mathcal{E}$ such that for any $k\in [n]$, $V^2_k=V^1_{k+1}$. A directed path begins as $V^1_1$ and ends at $V^2_k$.
\end{definition}

\begin{definition}[Directed acyclic graph]
A directed graph $\mathcal{G}$ is a directed acyclic graph if it contains no directed paths beginning and ending at the same node.
\end{definition}

\begin{definition}[Parents, ancestors, nondescendents]
Given a graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, the parents of a node $V_i$ are all the nodes $V_j$ such that there is an edge $(V_j, V_i)\in \mathcal{E}$: $\mathrm{Pa}(V_i)=\{V_j|(V_j,V_i)\in \sigalg{E}\}$.

The ancestors of $V_i$ are all nodes $V_j$ which are either parents of $V_i$ or parents of ancestors of $V_i$ (this is a recursive definition).

Nondescendents of $V_i$, $\mathrm{ND}(V_i)$, are nodes $V_j$ such that $V_i$ is not an ancestor of $V_j$.
\end{definition}

\begin{definition}[Model graph association]\label{def:mga}
Given a set of variables $(\RV{V}_i)_{i\in [k]}$, an \emph{associated} directed acyclic graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$ is a graph with a node $V_i$ for each variable $\RV{V}_i$. We define the parents of a variable via this association: $\mathrm{Pa}_\mathcal{G}(\RV{V}_i)=\{\RV{V}_j|(V_j,V_i)\in \sigalg{E}\}$.
\end{definition}

\todo{parameters}