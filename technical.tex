%!TEX root = main.tex

\section{Technical Prerequisites}\label{sec:tech_prereq}

Our approach to causal inference is based on probability theory. Many results and conventions will be familiar to readers, and these are collected in Section \ref{sec:standard_prob}. Because decision models are stochastic functions rather than probability measures (Section \ref{sec:probability_sets}), we make use a generalisation of conditional independence called \emph{extended conditional independence}, explained in Section \ref{sec:eci}.

Section \ref{sec:d_graphs} defines some standard terms relating to directed acyclic graphs, which are likely familiar to anyone acquainted with structural causal models.

\subsection{Probability Theory}\label{sec:standard_prob}

\subsubsection{Measurable spaces}

\begin{definition}[Sigma algebra]
Given a set $A$, a $\sigma$-algebra $\mathcal{A}$ is a collection of subsets of $A$ where
\begin{itemize}
	\item $A\in \mathcal{A}$ and $\emptyset\in \mathcal{A}$
	\item $B\in \mathcal{A}\implies B^{\complement}\in\mathcal{A}$
	\item $\mathcal{A}$ is closed under countable unions: For any countable collection $\{B_i|i\in Z\subset \mathbb{N}\}$ of elements of $\mathcal{A}$, $\cup_{i\in Z}B_i\in \mathcal{A}$ 
\end{itemize}
\end{definition}

\begin{definition}[Measurable space]
A measurable space $(A,\mathcal{A})$ is a set $A$ along with a $\sigma$-algebra $\mathcal{A}$.
\end{definition}

\begin{definition}[Sigma algebra generated by a set of events]
Given a set $A$ and an arbitrary collection of subsets $U\supset\mathscr{P}(A)$, the $\sigma$-algebra generated by $U$, $\sigma(U)$, is the smallest $\sigma$-algebra containing $U$.
\end{definition}

\paragraph{Common $\sigma$ algebras}

For any $A$, $\{\emptyset,A\}$ is a $\sigma$-algebra. In particular, it is the only sigma algebra for any one element set $\{*\}$.

For countable $A$, the power set $\mathscr{P}(A)$ is known as the discrete $\sigma$-algebra.

Given $A$ and a collection of subsets of $B\subset\mathscr{P}(A)$, $\sigma(B)$ is the smallest $\sigma$-algebra containing all the elements of $B$. 

If $A$ is a topological space with open sets $T$, $\mathcal{B}(\mathbb{R}):=\sigma(T)$ is the \emph{Borel $\sigma$-algebra} on $A$.

If $A$ is a separable, completely metrizable topological space, then $(A,\mathcal{B}(A))$ is a \emph{standard measurable set}. All standard measurable sets are isomorphic to either $(\mathbb{R},B(\mathbb{R}))$ or $(C,\mathscr{P}(C))$ for denumerable $C$ \citep[Chap. 1]{cinlar_probability_2011}.

\subsubsection{Probability measures and Markov kernels}

\begin{definition}[Probability measure]\label{def:prob_meas}
Given a measurable space $(E,\sigalg{E})$, a map $\mu:\sigalg{E}\to [0,1]$ is a \emph{probability measure} if
\begin{itemize}
	\item $\mu(E)=1$, $\mu(\emptyset)=0$
	\item Given countable collection $\{A_i\}\subset\mathscr{E}$, $\mu(\cup_{i} A_i) = \sum_i \mu(A_i)$
\end{itemize}
\end{definition}

\begin{definition}[Set of all probability measures]\label{no:prob_meas_set}
The set of all probability measures on $(E,\sigalg{E})$ is written $\Delta(E)$. We equip $\Delta(E)$ with the coarsest $\sigma$-algebra such that the evaluation maps $\eta_B:\nu\mapsto \nu(B)$ are measurable for all $B\in \sigalg{F}$.
\end{definition}

\begin{definition}[Probability space]
A probability space is a triple $(\mu,E,\sigalg{E})$ consisting of a probability measure and a measurable space.
\end{definition}

\begin{definition}[Markov kernel]\label{def:markov_kern}
Given measurable spaces $(E,\sigalg{E})$ and $(F,\sigalg{F})$, a \emph{Markov kernel} or \emph{stochastic function} is a map $\kernel{M}:E\times\sigalg{F}\to [0,1]$ such that
\begin{itemize}
	\item The map $\kernel{M}(A|\cdot):x\mapsto \kernel{M}(A|x)$ is $\sigalg{E}$-measurable for all $A\in \sigalg{F}$
	\item The map $\kernel{M}(\cdot|x):A\mapsto \kernel{M}(A|x)$ is a probability measure on $(F,\sigalg{F})$ for all $x\in E$
\end{itemize}
\end{definition}

\begin{notation}[Signature of a Markov kernel]
Given measurable spaces $(E,\sigalg{E})$ and $(F,\sigalg{F})$ and $\kernel{M}:E\times\sigalg{F}\to [0,1]$, we write the signature of $\kernel{M}:E\kto F$, read ``$\kernel{M}$ maps from $E$ to probability measures on $F$''.
\end{notation}

\begin{definition}[Deterministic Markov kernel]
A \emph{deterministic} Markov kernel $\kernel{A}:E\to \Delta(\mathcal{F})$ is a kernel such that $\kernel{A}_x(B)\in\{0,1\}$ for all $x\in E$, $B\in\mathcal{F}$.
\end{definition}

\paragraph{Common probability measures and Markov kernels}

\begin{definition}[Dirac measure]\label{def:dirac_meas}
The \emph{Dirac measure} $\delta_x\in \Delta(X)$ is a probability measure such that $\delta_x(A)=\llbracket x\in A \rrbracket$
\end{definition}

\begin{definition}[Markov kernel associated with a function]\label{def:mkern_func}
Given measurable $f:(X,\sigalg{X})\to (Y,\sigalg{Y})$, $\kernel{F}_f:X\kto Y$ is the Markov kernel given by $x\mapsto \delta_{f(x)}$
\end{definition}

\begin{definition}[Markov kernel associated with a probability measure]
Given $(X,\sigalg{X})$, a one-element measurable space $(\{*\},\{\{*\},\emptyset\})$ and a probability measure $\mu\in \Delta(X)$, the associated Markov kernel $\kernel{Q}_\mu:\{*\}\kto X$ is the unique Markov kernel $*\mapsto \mu$
\end{definition}

\subsubsection{Variables, conditionals and marginals}

\begin{definition}[Random variable]\label{def:variable}
Given a measurable space $(\Omega,\sigalg{F})$, which we refer to as a \emph{sample space}, and a measurable space of values $(X,\sigalg{X})$, an \emph{$X$-valued random variable on $\Omega$} is a measurable function $\RV{X}:(\Omega,\sigalg{F})\to (X,\sigalg{X})$.
\end{definition}

A sequence of random variables is also a random variable.

\begin{definition}[Sequence of variables]\label{def:seqvar}
Given a sample space $(\Omega,\sigalg{F})$ and two random variables $\RV{X}:(\Omega,\sigalg{F})\to (X,\sigalg{X})$, $\RV{Y}:(\Omega,\sigalg{F})\to (Y,\sigalg{Y})$, $(\RV{X},\RV{Y}):\Omega\to X\times Y$ is the random variable $\omega\mapsto (\RV{X}(\omega),\RV{Y}(\omega))$.
\end{definition}

We define a partial order on random variables such that $\RV{Y}$ is higher than $\RV{X}$ if $\RV{X}$ is given by application of a function to $\RV{Y}$. For example, $\RV{Y}\varlessthan (\RV{W},\RV{Y})$ as $\RV{Y}$ can be obtained by composing a projection with $(\RV{W},\RV{Y})$.

\begin{definition}[Random variables determined by another random variable]\label{def:variable_po}
Given a sample space $(\Omega,\sigalg{F})$ and variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$, $\RV{X}\varlessthan \RV{Y}$ if there is some $f:Y\to X$ such that $\RV{X}=f\circ \RV{Y}$.
\end{definition}

We use superscripts to specify marginal and conditional distributions, as subscrips (which are a somewhat more common notation) are reserved for specifying options in decision models (Section \ref{sec:probability_sets}).

\begin{definition}[Marginal distribution]\label{def:pushforward}
Given a probability space $(\mu,\Omega,\sigalg{F})$ and a variable $\RV{X}:\Omega\to (X,\sigalg{X})$, the \emph{marginal distribution} of $\RV{X}$ with respect to $\mu$, $\mu^{\RV{X}}:\sigalg{X}\to [0,1]$ by $\mu^{\RV{X}}(A):=\mu(\RV{X}^{-1}(A))$ for any $A\in \sigalg{X}$.
\end{definition}

\begin{definition}[Conditional distribution]\label{def:disint}
Given a probability space $(\mu,\Omega,\sigalg{F})$ and variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$, the \emph{conditional distribution} of $\RV{Y}$ given $\RV{X}$ is any Markov kernel $\mu^{\RV{Y}|\RV{X}}:X\kto Y$ such that
\begin{align}
	\mu^{\RV{XY}}(A\times B)&=\int_{A} \mu^{\RV{Y}|\RV{X}}(B|x) \mathrm{d}\mu^{\RV{X}}(x) &\forall A\in \sigalg{X}, B\in \sigalg{Y}
\end{align}
\end{definition}

\begin{definition}[Trivial variable]\label{no:single_valued}
We let $*$ stand for a single-valued variable $*:\Omega\to \{*\}$.
\end{definition}

% \subsubsection{Markov kernel product notation}\label{ssec:product_notation}

% Three pairwise \emph{product} operations involving Markov kernels can be defined: measure-kernel products, kernel-kernel products and kernel-function products. These are analagous to row vector-matrix products, matrix-matrix products and matrix-column vector products respectively.

% \begin{definition}[Measure-kernel product]
% Given $\mu\in \Delta(\mathcal{X})$ and $\kernel{M}:X\kto Y$, the \emph{measure-kernel product} $\mu\kernel{M}\in \Delta(Y)$ is given by
% \begin{align}
% \mu\kernel{M} (A) := \int_X \kernel{M}(A|x) \mu(\mathrm{d}x)
% \end{align}
% for all $A\in \sigalg{Y}$.
% \end{definition}

% \begin{definition}[Kernel-kernel product]\label{def:kproduct}
% Given $\kernel{M}:X\kto Y$ and $\kernel{N}:Y\kto Z$, the \emph{kernel-kernel product} $\kernel{M}\kernel{N}:X\kto Z$ is given by
% \begin{align}
% \kernel{MN} (A|x) := \int_Y \kernel{N}(A|x) \kernel{M}(\mathrm{d}y|x)
% \end{align}
% for all $A\in \sigalg{Z}$, $x\in X$.
% \end{definition}

% \begin{definition}[Kernel-function product]
% Given $\kernel{M}:X\kto Y$ and $f:Y\to Z$, the \emph{kernel-function product} $\kernel{M}f:X\to Z$ is given by
% \begin{align}
% \kernel{M}f (x) := \int_Y f(y)\kernel{N}(\mathrm{d}y|x)
% \end{align}
% for all $x\in X$.
% \end{definition}

% \begin{definition}[Tensor product]
% Given $\kernel{M}:X\kto Y$ and $\kernel{L}:W\kto Z$, the tensor product $\kernel{M}\otimes\kernel{N}:X\times W\kto Y\times Z$ is given by
% \begin{align}
% 	(\kernel{M}\otimes\kernel{L})(A\times B|x,w):=\kernel{M}(A|x)\kernel{L}(B|w)
% \end{align}
% For all $x\in X$, $w\in W$, $A\in \sigalg{Y}$ and $B\in \sigalg{Z}$.
% \end{definition}

% All products are associative \citep[Chapter 1]{cinlar_probability_2011}.

\subsection{Decision models}\label{sec:probability_sets}

A \emph{decision model} is a Markov kernel $\prob{P}_\cdot$ from an option set $(C,\sigalg{C})$ to a sample space $(\Omega,\sigalg{F})$.

\begin{definition}[Decision model]\label{def:dec_model}
A decision model is a triple $(\prob{P}_\cdot, (\Omega,\sigalg{F}), (C,\sigalg{C}))$ where $\prob{P}_\cdot:C\kto \Omega$ is a Markov kernel, $(\Omega,\sigalg{F})$ is the sample space and $(C,\sigalg{C})$ is the option set.
\end{definition}

For an option $\alpha\in C$, we say $\prob{P}_\alpha$ is the model $\prob{P}_\cdot$ evaluated at $\alpha$.

\begin{definition}[Almost sure equality]
Given a decision model $(\prob{P}_\cdot, (\Omega,\sigalg{F}), (C,\sigalg{C}))$ and random variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$, two Markov kernels $\kernel{K}:X\kto Y$ and $\kernel{L}:X\kto Y$ are $\prob{P}_\cdot,\RV{X},\RV{Y}$-almost surely equal if for all $A\in\sigalg{X}$, $B\in \sigalg{Y}$, $\alpha\in C$
\begin{align}
    \int_A \kernel{K}(B|x)\prob{P}_\alpha^{\RV{X}}(\mathrm{d}x) = \int_A\kernel{L}(B|x)\prob{P}_\alpha^{\RV{X}}(\mathrm{d}x)
\end{align}
we write this as $\kernel{K}\overset{\prob{P}_\cdot^{\RV{X}}}{\cong}\kernel{L}$.
\end{definition}

Equivalently, $\kernel{K}$ and $\kernel{L}$ are almost surely equal if the set $C:\{x|\exists B\in\sigalg{Y}:\kernel{K}(B|x)\neq\kernel{L}(B|x)\}$ has measure 0 with respect to $\prob{P}_\alpha^{\RV{X}}$ for all $\alpha\in C$.

\subsection{Extended conditional independence}\label{sec:eci}

Because decision models aren't standard probability spaces, we need some version of conditional independence for decision models. Such a notion has already been worked out in some detail: it is the idea of \emph{extended conditional independence} defined in \citet{constantinou_extended_2017}. Extended conditional independence is substantially more general than we need for our purposes, and in fact we only consider two special cases of it. However, we still make use of the notational convention introduced in that paper.

We will first define regular conditional independence. We define it in terms of a having a conditional that ``ignores one of its inputs'', which, provided conditional probabilities exist, is equivalent to other common definitions \cite{fritz_synthetic_2020}.

\begin{definition}[Conditional independence]\label{def:ci}
Given a decision model $(\prob{P}_\cdot, (\Omega,\sigalg{F}), (C,\sigalg{C}))$, variables $\RV{X},\RV{Y},\RV{Z}$ and fixing some $\alpha\in C$, we say $\RV{Y}$ is conditionally independent of $\RV{X}$ given $\RV{Z}$, written $\RV{Y}\CI_{\model{P}_{\alpha}}\RV{X}|\RV{Z}$, if there exists some $\kernel{K}:Z\kto Y$ such that
\begin{align}
    \prob{P}^{\RV{Y}|\RV{XZ}}(A|x,z) &\overset{\prob{P}_\alpha^{\RV{XZ}}}{\cong} \prob{K}(A|z)&\forall A\in \sigalg{Y}
\end{align}
\end{definition}

Extended conditional independence as introduced by \citet{constantinou_extended_2017} is defined using pairs of  ``complementary nonstochastic variables'' on the option set $C$.

\begin{definition}[Nonstochastic variable]\label{def:nonstoc_var}
Given a decision model $(\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C}))$ and a measurable set $(X,\sigalg{X})$, a nonstochastic variable is a measurable function $\phi:C\to X$.
\end{definition}

\begin{definition}[Complementary nonstochastic variables]\label{def:comp_var}
A pair of nonstochastic variables $\phi$ and $\xi$ are complementary if the pair $(\phi,\xi)$ is invertible.
\end{definition}

Unlike \citet{constantinou_extended_2017}, we limit ourselves to a definition of extended conditional independence where regular uniform conditional probabilities exist. Our definition is otherwise identical.

\begin{definition}[Extended conditional independence]\label{def:eci_orig}
Given a probability set $\prob{P}_C$, variables $\RV{X}$, $\RV{Y}$ and $\RV{Z}$ and complementary nonstochastic variables $\phi$ and $\xi$, the extended conditional independence $\RV{Y}\CI^e_{\prob{P}_\cdot} \RV{X} \phi|\RV{Z} \xi$ holds if for each $a\in \xi(C)$, $\alpha,\alpha'\in \xi^{-1}(a)$,
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{XZ}} &\overset{\prob{P}_{\cdot}}{\cong} \prob{P}_{\alpha'}^{\RV{Y}|\RV{XZ}}
\end{align}
and for all $\alpha\in C$
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{XZ}}(A|x,z) &\overset{\prob{P}_{\alpha}}{\cong} \prob{P}_{\alpha}^{\RV{Y}|\RV{Z}}(A|z)&\forall A\in \sigalg{Y},(x,z)\in X\times Z\label{eq:eci}
\end{align}
\end{definition}

In this work we only ever consider the complimentary pair $(\mathrm{id}_C,*)$ where $*$ is the trivial variable $\cdot \mapsto *$., in which case extended conditional independence breaks down into two special cases: \emph{global conditional independence} and \emph{uniform conditional independence}. The former can be understood as ``conditional independence for every $\alpha\in C$'', while the latter means ``conditional independence for every $\alpha\in C$, and moreover conditionally independent of $\mathrm{id}_C$''.

\begin{definition}[Global conditional independence]\label{def:eci_glob}
Given a decision model $(\prob{P}_\cdot, (\Omega,\sigalg{F}), (C,\sigalg{C}))$ and variables $\RV{X}$, $\RV{Y}$ and $\RV{Z}$, $\RV{Y}$ is globally independent of $\RV{X}$ given $\RV{Z}$, written $\RV{Y}\CI^e_{\prob{P}_\cdot} \RV{X} |(\RV{Z}, \mathrm{id}_C)$ if for each $\alpha\in C$
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{XZ}}(A|x,z) &\overset{\prob{P}_{\alpha}^{\RV{XZ}}}{\cong} \prob{P}_{\alpha}^{\RV{Y}|\RV{Z}}(A|z)&\forall A\in \sigalg{Y},(x,z)\in X\times Z\label{eq:gci}
\end{align}
\end{definition}

\begin{definition}[Uniform conditional independence]\label{def:eci}
Given a decision model $(\prob{P}_\cdot, (\Omega,\sigalg{F}), (C,\sigalg{C}))$ and variables $\RV{X}$, $\RV{Y}$ and $\RV{Z}$, the uniform conditional independence $\RV{Y}\CI^e_{\prob{P}_\cdot} (\RV{X}, \mathrm{id}_C)|\RV{Z}$ holds if $\RV{Y}\CI^e_{\prob{P}_\cdot} \RV{X} |(\RV{Z}, \mathrm{id}_C)$ and furthermore for all $\alpha,\alpha'\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{XZ}} &\overset{\prob{P}_\alpha^{\RV{XZ}}}{\cong} \prob{P}_{\alpha'}^{\RV{Y}|\RV{XZ}}\label{eq:uci}
\end{align}
\end{definition}

For countable sets $C$ we can reason with collections of extended conditional independence statements as if they were regular conditional independence statements. In the following rules, $\phi$ and $\xi$ refer to complementary variables on the set $C$ (see \citet{constantinou_extended_2017} for details), but for our purposes we only consider the cases where either $\phi=\mathrm{id}_C$ and $\xi=*$ or $\phi=*$ and $\xi=\mathrm{id}_C$. In the rest of this text, we will omit the trivial variable from extended conditional independence statements.

\begin{enumerate}
    \item Symmetry: $\RV{X}\CI_{\prob{P}_{\cdot}}^e (\RV{Y}, \phi)|(\RV{Z}, \xi)$ iff $\RV{Y}\CI_{\prob{P}_{\cdot}}^e (\RV{X}, \phi)|(\RV{Z},\xi)$
    \item $\RV{X}\CI_{\prob{P}_{\cdot}}^e (\RV{Y}, \mathrm{id}_C)| (\RV{Y}, \mathrm{id}_C)$
    \item Decomposition: $\RV{X}\CI_{\prob{P}_{\cdot}}^e (\RV{Y}, \phi)|\RV{W}\xi$ and $\RV{Z}\varlessthan\RV{Y}$ implies $\RV{X}\CI_{\prob{P}_{\cdot}}^e(\RV{Z},\phi)|(\RV{W},\xi)$
    \item Weak union:
    \begin{enumerate}
     	\item $\RV{X}\CI^e_{\prob{P}_{\cdot}} (\RV{Y}, \phi)|(\RV{W}, \xi)$ and $\RV{Z}\varlessthan \RV{Y}$ implies $\RV{X}\CI_{\prob{P}_{\cdot}}^e(\RV{Y},\phi)|(\RV{Z},\RV{W}, \xi)$
     	\item $\RV{X}\CI_{\prob{P}_{\cdot}}^e \RV{Y} \mathrm{id}_{C}|\RV{W}$ implies $\RV{X}\CI_{\prob{P}_{\cdot}}^e\RV{Y}|(\RV{W},\mathrm{id}_C)$
     \end{enumerate} 
    \item Contraction: $\RV{X}\CI_{\prob{P}_{\cdot}}^e(\RV{Z},phi)|(\RV{W},\xi)$ and $\RV{X}\CI_{\prob{P}_{\cdot}}^e(\RV{Y},\phi)|(\RV{Z},\RV{W})\xi$ implies $\RV{X}\CI_{\prob{P}_{\cdot}}^e(\RV{Y},\RV{Z},\phi)|(\RV{W},\xi)$
\end{enumerate} 

If we have the extended conditional independence $\RV{Y}\CI^e_{\prob{P}_\cdot} \mathrm{id}_C | \RV{X}$, then by definition for all $\alpha,\alpha'\in C$ we have $\prob{P}_\alpha^{\RV{Y}|\RV{X}}=\prob{P}_{\alpha'}^{\RV{Y}|\RV{X}}$. In this case, we use the notation $\prob{P}_C^{\RV{Y}|\RV{X}}$ to indicate that the conditional distribution does not depend on the choice of $\alpha$

\begin{definition}[Uniform conditional distribution]\label{def:uci}
Given a decision model $(\prob{P}_\cdot, (\Omega,\sigalg{F}), (C,\sigalg{C}))$ and variables $\RV{X}$, $\RV{Y}$, if $\RV{Y}\CI^e_{\prob{P}_\cdot} \mathrm{id}_C | \RV{X}$ then
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{X}} &= \prob{P}_\alpha^{\RV{Y}|\RV{X}}
\end{align}
for any $\alpha\in C$. If $\RV{Y}\not \CI^e_{\prob{P}_\cdot} \mathrm{id}_C | \RV{X}$ then $\prob{P}_C^{\RV{Y}|\RV{X}}$ is not defined.
\end{definition}

\subsection{Directed graphs}\label{sec:d_graphs}


\begin{definition}[Directed graph]
A directed acyclic graph $\mathcal{G}$ is a set of nodes $\mathcal{V}$ and a set of edges $\mathcal{E}$. Each edge is an ordered pair of nodes $(V_i,V_j)\in \mathcal{V}^2$, with $V_i$ the source and $V_j$ the destination. An acyclic graph must have no directed path that begins and ends at $V_i$ for any $V_i\in\mathcal{V}$.
\end{definition}

\begin{definition}[Directed path]
Given a graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, a directed path is a sequence of edges $((V^1_{k},V^2_{k}))_{k\in [n]}$ from $\mathcal{E}$ such that for any $k\in [n]$, $V^2_k=V^1_{k+1}$. A directed path begins as $V^1_1$ and ends at $V^2_k$.
\end{definition}

\begin{definition}[Directed acyclic graph]
A directed graph $\mathcal{G}$ is a directed acyclic graph if it contains no directed paths beginning and ending at the same node.
\end{definition}

\begin{definition}[Parents]
Given a graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, the parents of a node $V_i$ are all the nodes $V_j$ such that there is an edge $(V_j, V_i)\in \mathcal{E}$: $\mathrm{Pa}(V_i)=\{V_j|(V_j,V_i)\in \sigalg{E}\}$.
\end{definition}

\begin{definition}[Model graph association]\label{def:mga}
Given a set of variables $(\RV{V}_i)_{i\in [k]}$, an \emph{associated} directed acyclic graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$ is a graph with a node $V_i$ for each variable $\RV{V}_i$. We define the parents of a variable via this association: $\mathrm{Pa}_\mathcal{G}(\RV{V}_i)=\{\RV{V}_j|(V_j,V_i)\in \sigalg{E}\}$.
\end{definition}

\todo{parameters}